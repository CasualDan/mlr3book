<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>3 Model Optimization | mlr3 book</title>
<meta name="author" content="Marc Becker">
<meta name="author" content="Martin Binder">
<meta name="author" content="Bernd Bischl">
<meta name="author" content="Michel Lang">
<meta name="author" content="Florian Pfisterer">
<meta name="author" content="Nicholas G. Reich">
<meta name="author" content="Jakob Richter">
<meta name="author" content="Patrick Schratz">
<meta name="author" content="Raphael Sonabend">
<meta name="author" content="Damir Pulatov">
<meta name="description" content="Model Tuning Machine learning algorithms have default values set for their hyperparameters. Irrespective, these hyperparameters need to be changed by the user to achieve optimal performance on the...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="3 Model Optimization | mlr3 book">
<meta property="og:type" content="book">
<meta property="og:url" content="https://mlr3book.mlr-org.com/optimization.html">
<meta property="og:image" content="https://mlr3book.mlr-org.com//block.png">
<meta property="og:description" content="Model Tuning Machine learning algorithms have default values set for their hyperparameters. Irrespective, these hyperparameters need to be changed by the user to achieve optimal performance on the...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3 Model Optimization | mlr3 book">
<meta name="twitter:description" content="Model Tuning Machine learning algorithms have default values set for their hyperparameters. Irrespective, these hyperparameters need to be changed by the user to achieve optimal performance on the...">
<meta name="twitter:image" content="https://mlr3book.mlr-org.com//block.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">mlr3 book</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Quickstart</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction and Overview</a></li>
<li><a class="" href="basics.html"><span class="header-section-number">2</span> Basics</a></li>
<li><a class="active" href="optimization.html"><span class="header-section-number">3</span> Model Optimization</a></li>
<li><a class="" href="pipelines.html"><span class="header-section-number">4</span> Pipelines</a></li>
<li><a class="" href="technical.html"><span class="header-section-number">5</span> Technical</a></li>
<li><a class="" href="extending.html"><span class="header-section-number">6</span> Extending</a></li>
<li><a class="" href="special-tasks.html"><span class="header-section-number">7</span> Special Tasks</a></li>
<li><a class="" href="interpretation.html"><span class="header-section-number">8</span> Model Interpretation</a></li>
<li><a class="" href="appendix.html"><span class="header-section-number">9</span> Appendix</a></li>
<li><a class="" href="citation-info.html">Citation Info</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/mlr-org/mlr3book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="optimization" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Model Optimization<a class="anchor" aria-label="anchor" href="#optimization"><i class="fas fa-link"></i></a>
</h1>
<p><strong>Model Tuning</strong></p>
<p>Machine learning algorithms have default values set for their hyperparameters.
Irrespective, these hyperparameters need to be changed by the user to achieve optimal performance on the given dataset.
A manual selection of hyperparameter values is not recommended as this approach rarely leads to the best performance.
To substantiate the validity of the selected hyperparameters (= <a href="optimization.html#tuning">tuning</a>), data-driven optimization is recommended.
In order to tune a machine learning algorithm, one has to specify (1) the <a href="optimization.html#tuning-optimization">search space</a>, (2) the <a href="optimization.html#tuning-algorithms">optimization algorithm</a> (aka tuning method), (3) an evaluation method, i.e., a resampling strategy and (4) a performance measure.</p>
<p>In summary, the sub-chapter on <a href="optimization.html#tuning">tuning</a> illustrates how to:</p>
<ul>
<li>undertake empirically sound <a href="optimization.html#tuning">hyperparameter selection</a>
</li>
<li>select the <a href="optimization.html#tuning-optimization">optimizing algorithm</a>
</li>
<li>write out <a href="optimization.html#searchspace">search spaces concisely</a>
</li>
<li>
<a href="optimization.html#tuning-triggering">trigger</a> the tuning</li>
<li>
<a href="optimization.html#autotuner">automate</a> tuning</li>
</ul>
<p>This sub-chapter also requires the package <a href="https://mlr3tuning.mlr-org.com">mlr3tuning</a>, an extension package which supports hyperparameter tuning.</p>
<p><strong>Feature Selection</strong></p>
<p>The second part of this chapter explains <a href="optimization.html#fs">feature selection</a>, also known as variable selection.
<a href="optimization.html#fs">Feature selection</a> is the process of finding a subset of relevant features of the data.
Some of the reasons to perform the selection:</p>
<ul>
<li>enhance the interpretability of the model,</li>
<li>speed up model fitting or</li>
<li>improve the learner performance by reducing noise in the data.</li>
</ul>
<p>In this book we focus mainly on the last aspect.
Different approaches exist to identify the relevant features.
In the sub-chapter on <a href="optimization.html#fs">feature selection</a>, we emphasize three methods:</p>
<ul>
<li>
<a href="optimization.html#fs-filter">Filter</a> algorithms select features independently of the learner according to a score.</li>
<li>
<a href="optimization.html#fs-var-imp-filters">Variable importance filters</a> select features that are important according to a learner.</li>
<li>
<a href="optimization.html#fs-wrapper">Wrapper methods</a> iteratively select features to optimize a performance measure.</li>
</ul>
<p>Note, that filters do not require a learner.
<em>Variable importance filters</em> require a learner that can calculate feature importance values once it is trained.
The obtained importance values can be used to subset the data, which can then be used to train a learner.
<em>Wrapper methods</em> can be used with any learner but need to train the learner multiple times.</p>
<p><strong>Nested Resampling</strong></p>
<p>In order to get a good estimate of generalization performance and avoid data leakage, both an outer (performance) and an inner (tuning/feature selection) resampling process are necessary.
The following features are discussed in this chapter:</p>
<ul>
<li>
<a href="optimization.html#nested-resampling">Inner and outer resampling strategies</a> in nested resampling</li>
<li>The <a href="optimization.html#nested-resamp-exec">execution</a> of nested resampling</li>
<li>The <a href="optimization.html#nested-resamp-eval">evaluation</a> of executed resampling iterations</li>
</ul>
<p>This sub-chapter will provide instructions on how to implement nested resampling, accounting for both inner and outer resampling in <a href="https://mlr3.mlr-org.com">mlr3</a>.</p>

<div id="tuning" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Hyperparameter Tuning<a class="anchor" aria-label="anchor" href="#tuning"><i class="fas fa-link"></i></a>
</h2>
<p>Hyperparameters are second-order parameters of machine learning models that, while often not explicitly optimized during the model estimation process, can have an important impact on the outcome and predictive performance of a model.
Typically, hyperparameters are fixed before training a model.
However, because the output of a model can be sensitive to the specification of hyperparameters, it is often recommended to make an informed decision about which hyperparameter settings may yield better model performance.
In many cases, hyperparameter settings may be chosen <em>a priori</em>, but it can be advantageous to try different settings before fitting your model on the training data.
This process is often called model ‘tuning’.</p>
<p>Hyperparameter tuning is supported via the <a href="https://mlr3tuning.mlr-org.com">mlr3tuning</a> extension package.
Below you can find an illustration of the process:</p>
<div class="inline-figure"><img src="images/tuning_process.svg" style="display: block; margin: auto;"></div>
<p>At the heart of <a href="https://mlr3tuning.mlr-org.com">mlr3tuning</a> are the R6 classes:</p>
<ul>
<li>
<a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a>, <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html"><code>TuningInstanceMultiCrit</code></a>: These two classes describe the tuning problem and store the results.</li>
<li>
<a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a>: This class is the base class for implementations of tuning algorithms.</li>
</ul>
<div id="tuning-optimization" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> The <code>TuningInstance*</code> Classes<a class="anchor" aria-label="anchor" href="#tuning-optimization"><i class="fas fa-link"></i></a>
</h3>
<p>The following sub-section examines the optimization of a simple classification tree on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html"><code>Pima Indian Diabetes</code></a> data set.</p>
<div class="sourceCode" id="cb190"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3verse.mlr-org.com">"mlr3verse"</a></span><span class="op">)</span>
<span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"pima"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">task</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;TaskClassif:pima&gt; (768 x 9)
## * Target: diabetes
## * Properties: twoclass
## * Features (8):
##   - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,
##     triceps</code></pre>
<p>We use the classification tree from <a href="https://cran.r-project.org/package=rpart">rpart</a> and choose a subset of the hyperparameters we want to tune.
This is often referred to as the “tuning space”.</p>
<div class="sourceCode" id="cb192"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##                 id    class lower upper nlevels        default value
##  1:             cp ParamDbl     0     1     Inf           0.01      
##  2:     keep_model ParamLgl    NA    NA       2          FALSE      
##  3:     maxcompete ParamInt     0   Inf     Inf              4      
##  4:       maxdepth ParamInt     1    30      30             30      
##  5:   maxsurrogate ParamInt     0   Inf     Inf              5      
##  6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;      
##  7:       minsplit ParamInt     1   Inf     Inf             20      
##  8: surrogatestyle ParamInt     0     1       2              0      
##  9:   usesurrogate ParamInt     0     2       3              2      
## 10:           xval ParamInt     0   Inf     Inf             10     0</code></pre>
<p>Here, we opt to tune two parameters:</p>
<ul>
<li>The complexity <code>cp</code>
</li>
<li>The termination criterion <code>minsplit</code>
</li>
</ul>
<p>The tuning space needs to be bounded, therefore one has to set lower and upper bounds:</p>
<div class="sourceCode" id="cb194"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cp <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0.001</span>, upper <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,
  minsplit <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_int</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="op">)</span>
<span class="va">search_space</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##          id    class lower upper nlevels        default value
## 1:       cp ParamDbl 0.001   0.1     Inf &lt;NoDefault[3]&gt;      
## 2: minsplit ParamInt 1.000  10.0      10 &lt;NoDefault[3]&gt;</code></pre>
<p>Next, we need to specify how to evaluate the performance.
For this, we need to choose a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>resampling strategy</code></a> and a <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>performance measure</code></a>.</p>
<div class="sourceCode" id="cb196"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">hout</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>
<span class="va">measure</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span></code></pre></div>
<p>Finally, one has to select the budget available, to solve this tuning instance.
This is done by selecting one of the available <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminators</code></a>:</p>
<ul>
<li>Terminate after a given time (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_clock_time.html"><code>TerminatorClockTime</code></a>)</li>
<li>Terminate after a given amount of iterations (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_evals.html"><code>TerminatorEvals</code></a>)</li>
<li>Terminate after a specific performance is reached (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_perf_reached.html"><code>TerminatorPerfReached</code></a>)</li>
<li>Terminate when tuning does not improve (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_stagnation.html"><code>TerminatorStagnation</code></a>)</li>
<li>A combination of the above in an <em>ALL</em> or <em>ANY</em> fashion (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html"><code>TerminatorCombo</code></a>)</li>
</ul>
<p>For this short introduction, we specify a budget of 20 evaluations and then put everything together into a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a>:</p>
<div class="sourceCode" id="cb197"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3tuning.mlr-org.com">"mlr3tuning"</a></span><span class="op">)</span></code></pre></div>
<pre><code>## Loading required package: paradox</code></pre>
<div class="sourceCode" id="cb199"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">evals20</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span>

<span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html">TuningInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  task <span class="op">=</span> <span class="va">task</span>,
  learner <span class="op">=</span> <span class="va">learner</span>,
  resampling <span class="op">=</span> <span class="va">hout</span>,
  measure <span class="op">=</span> <span class="va">measure</span>,
  search_space <span class="op">=</span> <span class="va">search_space</span>,
  terminator <span class="op">=</span> <span class="va">evals20</span>
<span class="op">)</span>
<span class="va">instance</span></code></pre></div>
<pre><code>## &lt;TuningInstanceSingleCrit&gt;
## * State:  Not optimized
## * Objective: &lt;ObjectiveTuning:classif.rpart_on_pima&gt;
## * Search Space:
## &lt;ParamSet&gt;
##          id    class lower upper nlevels        default value
## 1:       cp ParamDbl 0.001   0.1     Inf &lt;NoDefault[3]&gt;      
## 2: minsplit ParamInt 1.000  10.0      10 &lt;NoDefault[3]&gt;      
## * Terminator: &lt;TerminatorEvals&gt;
## * Terminated: FALSE
## * Archive:
## &lt;ArchiveTuning&gt;
## Null data.table (0 rows and 0 cols)</code></pre>
<p>To start the tuning, we still need to select how the optimization should take place.
In other words, we need to choose the <strong>optimization algorithm</strong> via the <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a> class.</p>
</div>
<div id="tuning-algorithms" class="section level3" number="3.1.2">
<h3>
<span class="header-section-number">3.1.2</span> The <code>Tuner</code> Class<a class="anchor" aria-label="anchor" href="#tuning-algorithms"><i class="fas fa-link"></i></a>
</h3>
<p>The following algorithms are currently implemented in <a href="https://mlr3tuning.mlr-org.com">mlr3tuning</a>:</p>
<ul>
<li>Grid Search (<a href="https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html"><code>TunerGridSearch</code></a>)</li>
<li>Random Search (<a href="https://mlr3tuning.mlr-org.com/reference/mlr_tuners_random_search.html"><code>TunerRandomSearch</code></a>) <span class="citation">(<a href="references.html#ref-bergstra2012" role="doc-biblioref">Bergstra and Bengio 2012</a>)</span>
</li>
<li>Generalized Simulated Annealing (<a href="https://mlr3tuning.mlr-org.com/reference/mlr_tuners_gensa.html"><code>TunerGenSA</code></a>)</li>
<li>Non-Linear Optimization (<a href="https://mlr3tuning.mlr-org.com/reference/mlr_tuners_nloptr.html"><code>TunerNLoptr</code></a>)</li>
</ul>
<p>In this example, we will use a simple grid search with a grid resolution of 5.</p>
<div class="sourceCode" id="cb201"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"grid_search"</span>, resolution <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></code></pre></div>
<p>Since we have only numeric parameters, <a href="https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html"><code>TunerGridSearch</code></a> will create an equidistant grid between the respective upper and lower bounds.
As we have two hyperparameters with a resolution of 5, the two-dimensional grid consists of <span class="math inline">\(5^2 = 25\)</span> configurations.
Each configuration serves as a hyperparameter setting for the previously defined <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> which is then fitted on the task using the provided <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a>.
All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminator</code></a> signals that the budget is exhausted.</p>
</div>
<div id="tuning-triggering" class="section level3" number="3.1.3">
<h3>
<span class="header-section-number">3.1.3</span> Triggering the Tuning<a class="anchor" aria-label="anchor" href="#tuning-triggering"><i class="fas fa-link"></i></a>
</h3>
<p>To start the tuning, we simply pass the <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> to the <code>$optimize()</code> method of the initialized <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a>.
The tuner proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li>The <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a> proposes at least one hyperparameter configuration (the <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a> may propose multiple points to improve parallelization, which can be controlled via the setting <code>batch_size</code>).</li>
<li>For each configuration, the given <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> is fitted on the <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a> using the provided <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a>.
All evaluations are stored in the archive of the <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a>.</li>
<li>The <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminator</code></a> is queried if the budget is exhausted.
If the budget is not exhausted, restart with 1) until it is.</li>
<li>Determine the configuration with the best observed performance.</li>
<li>Store the best configurations as result in the instance object.
The best hyperparameter settings (<code>$result_learner_param_vals</code>) and the corresponding measured performance (<code>$result_y</code>) can be accessed from the instance.</li>
</ol>
<div class="sourceCode" id="cb202"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></code></pre></div>
<pre><code>## INFO  [12:04:28.086] [bbotk] Starting to optimize 2 parameter(s) with '&lt;OptimizerGridSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=20, k=0]' 
## INFO  [12:04:28.134] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:28.336] [bbotk] Result of batch 1: 
## INFO  [12:04:28.338] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:28.338] [bbotk]  0.02575        5     0.2578            0.018 
## INFO  [12:04:28.338] [bbotk]                                 uhash 
## INFO  [12:04:28.338] [bbotk]  f190f139-ee9c-4bfa-a26a-c2355b86ae93 
## INFO  [12:04:28.340] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:28.458] [bbotk] Result of batch 2: 
## INFO  [12:04:28.460] [bbotk]     cp minsplit classif.ce runtime_learners 
## INFO  [12:04:28.460] [bbotk]  0.001        1     0.3086            0.013 
## INFO  [12:04:28.460] [bbotk]                                 uhash 
## INFO  [12:04:28.460] [bbotk]  4df49a4d-e6cf-4b7d-8da4-3794d5b6a7f1 
## INFO  [12:04:28.461] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:28.565] [bbotk] Result of batch 3: 
## INFO  [12:04:28.567] [bbotk]      cp minsplit classif.ce runtime_learners 
## INFO  [12:04:28.567] [bbotk]  0.0505        8     0.2891            0.009 
## INFO  [12:04:28.567] [bbotk]                                 uhash 
## INFO  [12:04:28.567] [bbotk]  26d17200-7ddb-4f6f-bc4b-3ffa287b1d07 
## INFO  [12:04:28.569] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:28.675] [bbotk] Result of batch 4: 
## INFO  [12:04:28.677] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash 
## INFO  [12:04:28.677] [bbotk]  0.1       10     0.2891             0.01 bf98f46b-db87-4441-a7d4-c58cfcd4a179 
## INFO  [12:04:28.679] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:28.784] [bbotk] Result of batch 5: 
## INFO  [12:04:28.786] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:28.786] [bbotk]  0.02575       10     0.2578            0.009 
## INFO  [12:04:28.786] [bbotk]                                 uhash 
## INFO  [12:04:28.786] [bbotk]  eb48c123-9eb3-4057-bd6a-c80a1a6265cb 
## INFO  [12:04:28.788] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:28.897] [bbotk] Result of batch 6: 
## INFO  [12:04:28.899] [bbotk]     cp minsplit classif.ce runtime_learners 
## INFO  [12:04:28.899] [bbotk]  0.001        5     0.2773            0.012 
## INFO  [12:04:28.899] [bbotk]                                 uhash 
## INFO  [12:04:28.899] [bbotk]  09a84fcf-ab54-49fe-83b7-d89fe1b2754d 
## INFO  [12:04:28.901] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.017] [bbotk] Result of batch 7: 
## INFO  [12:04:29.019] [bbotk]      cp minsplit classif.ce runtime_learners 
## INFO  [12:04:29.019] [bbotk]  0.0505        1     0.2891            0.011 
## INFO  [12:04:29.019] [bbotk]                                 uhash 
## INFO  [12:04:29.019] [bbotk]  13332517-526b-464a-9c36-43780589a20c 
## INFO  [12:04:29.021] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.134] [bbotk] Result of batch 8: 
## INFO  [12:04:29.136] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash 
## INFO  [12:04:29.136] [bbotk]  0.1        8     0.2891            0.011 bdbcb5d8-3bb4-4adf-9969-bdafdd207412 
## INFO  [12:04:29.138] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.342] [bbotk] Result of batch 9: 
## INFO  [12:04:29.344] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:29.344] [bbotk]  0.07525       10     0.2891            0.012 
## INFO  [12:04:29.344] [bbotk]                                 uhash 
## INFO  [12:04:29.344] [bbotk]  eabd9347-bedd-430c-ac72-b8187b39277b 
## INFO  [12:04:29.345] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.454] [bbotk] Result of batch 10: 
## INFO  [12:04:29.456] [bbotk]     cp minsplit classif.ce runtime_learners 
## INFO  [12:04:29.456] [bbotk]  0.001        3     0.2891            0.013 
## INFO  [12:04:29.456] [bbotk]                                 uhash 
## INFO  [12:04:29.456] [bbotk]  cb945391-2007-4c8c-875e-d37ba6b789e6 
## INFO  [12:04:29.457] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.562] [bbotk] Result of batch 11: 
## INFO  [12:04:29.565] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash 
## INFO  [12:04:29.565] [bbotk]  0.1        5     0.2891            0.017 ffd3d3e2-5550-4849-ab04-6e2834231846 
## INFO  [12:04:29.566] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.668] [bbotk] Result of batch 12: 
## INFO  [12:04:29.671] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:29.671] [bbotk]  0.07525        1     0.2891             0.01 
## INFO  [12:04:29.671] [bbotk]                                 uhash 
## INFO  [12:04:29.671] [bbotk]  78d49191-e8ef-4cb9-9ec2-9ae41abe791a 
## INFO  [12:04:29.672] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.790] [bbotk] Result of batch 13: 
## INFO  [12:04:29.792] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash 
## INFO  [12:04:29.792] [bbotk]  0.1        3     0.2891            0.011 1f28e57b-54ed-4dc1-9f69-f8dec555dce2 
## INFO  [12:04:29.794] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:29.906] [bbotk] Result of batch 14: 
## INFO  [12:04:29.909] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash 
## INFO  [12:04:29.909] [bbotk]  0.1        1     0.2891            0.011 3441f789-6dba-4d23-beef-7b861de27f18 
## INFO  [12:04:29.911] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:30.026] [bbotk] Result of batch 15: 
## INFO  [12:04:30.028] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:30.028] [bbotk]  0.02575        1     0.2578             0.01 
## INFO  [12:04:30.028] [bbotk]                                 uhash 
## INFO  [12:04:30.028] [bbotk]  971658d6-54db-41bd-8b18-f4de4ecd34ff 
## INFO  [12:04:30.030] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:30.145] [bbotk] Result of batch 16: 
## INFO  [12:04:30.147] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:30.147] [bbotk]  0.02575        8     0.2578            0.012 
## INFO  [12:04:30.147] [bbotk]                                 uhash 
## INFO  [12:04:30.147] [bbotk]  30ed6670-a958-466b-bd8e-1a59937bf965 
## INFO  [12:04:30.149] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:30.259] [bbotk] Result of batch 17: 
## INFO  [12:04:30.261] [bbotk]      cp minsplit classif.ce runtime_learners 
## INFO  [12:04:30.261] [bbotk]  0.0505        5     0.2891             0.01 
## INFO  [12:04:30.261] [bbotk]                                 uhash 
## INFO  [12:04:30.261] [bbotk]  4081e1cf-42d3-40a0-a3a2-5eafcbb94827 
## INFO  [12:04:30.262] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:30.367] [bbotk] Result of batch 18: 
## INFO  [12:04:30.369] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:30.369] [bbotk]  0.07525        5     0.2891             0.01 
## INFO  [12:04:30.369] [bbotk]                                 uhash 
## INFO  [12:04:30.369] [bbotk]  9bf5a2ca-1159-4c9f-8dc4-8653ccaa7c4e 
## INFO  [12:04:30.371] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:30.501] [bbotk] Result of batch 19: 
## INFO  [12:04:30.503] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:30.503] [bbotk]  0.02575        3     0.2578            0.013 
## INFO  [12:04:30.503] [bbotk]                                 uhash 
## INFO  [12:04:30.503] [bbotk]  d1f72d10-0a45-45bf-83a9-1a9bdc339a67 
## INFO  [12:04:30.505] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:30.611] [bbotk] Result of batch 20: 
## INFO  [12:04:30.614] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:30.614] [bbotk]  0.07525        8     0.2891             0.01 
## INFO  [12:04:30.614] [bbotk]                                 uhash 
## INFO  [12:04:30.614] [bbotk]  43ab6e23-e230-4a47-99a3-76ad2a093eea 
## INFO  [12:04:30.621] [bbotk] Finished optimizing after 20 evaluation(s) 
## INFO  [12:04:30.623] [bbotk] Result: 
## INFO  [12:04:30.625] [bbotk]       cp minsplit learner_param_vals  x_domain classif.ce 
## INFO  [12:04:30.625] [bbotk]  0.02575        5          &lt;list[3]&gt; &lt;list[2]&gt;     0.2578</code></pre>
<pre><code>##         cp minsplit learner_param_vals  x_domain classif.ce
## 1: 0.02575        5          &lt;list[3]&gt; &lt;list[2]&gt;     0.2578</code></pre>
<div class="sourceCode" id="cb205"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result_learner_param_vals</span></code></pre></div>
<pre><code>## $xval
## [1] 0
## 
## $cp
## [1] 0.02575
## 
## $minsplit
## [1] 5</code></pre>
<div class="sourceCode" id="cb207"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result_y</span></code></pre></div>
<pre><code>## classif.ce 
##     0.2578</code></pre>
<p>One can investigate all resamplings which were undertaken, as they are stored in the archive of the <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> and can be accessed by using <code><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table()</a></code>:</p>
<div class="sourceCode" id="cb209"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span></code></pre></div>
<pre><code>##          cp minsplit classif.ce x_domain_cp x_domain_minsplit runtime_learners
##  1: 0.02575        5     0.2578     0.02575                 5            0.018
##  2: 0.00100        1     0.3086     0.00100                 1            0.013
##  3: 0.05050        8     0.2891     0.05050                 8            0.009
##  4: 0.10000       10     0.2891     0.10000                10            0.010
##  5: 0.02575       10     0.2578     0.02575                10            0.009
##  6: 0.00100        5     0.2773     0.00100                 5            0.012
##  7: 0.05050        1     0.2891     0.05050                 1            0.011
##  8: 0.10000        8     0.2891     0.10000                 8            0.011
##  9: 0.07525       10     0.2891     0.07525                10            0.012
## 10: 0.00100        3     0.2891     0.00100                 3            0.013
## 11: 0.10000        5     0.2891     0.10000                 5            0.017
## 12: 0.07525        1     0.2891     0.07525                 1            0.010
## 13: 0.10000        3     0.2891     0.10000                 3            0.011
## 14: 0.10000        1     0.2891     0.10000                 1            0.011
## 15: 0.02575        1     0.2578     0.02575                 1            0.010
## 16: 0.02575        8     0.2578     0.02575                 8            0.012
## 17: 0.05050        5     0.2891     0.05050                 5            0.010
## 18: 0.07525        5     0.2891     0.07525                 5            0.010
## 19: 0.02575        3     0.2578     0.02575                 3            0.013
## 20: 0.07525        8     0.2891     0.07525                 8            0.010
##               timestamp batch_nr      resample_result
##  1: 2021-10-05 12:04:28        1 &lt;ResampleResult[20]&gt;
##  2: 2021-10-05 12:04:28        2 &lt;ResampleResult[20]&gt;
##  3: 2021-10-05 12:04:28        3 &lt;ResampleResult[20]&gt;
##  4: 2021-10-05 12:04:28        4 &lt;ResampleResult[20]&gt;
##  5: 2021-10-05 12:04:28        5 &lt;ResampleResult[20]&gt;
##  6: 2021-10-05 12:04:28        6 &lt;ResampleResult[20]&gt;
##  7: 2021-10-05 12:04:29        7 &lt;ResampleResult[20]&gt;
##  8: 2021-10-05 12:04:29        8 &lt;ResampleResult[20]&gt;
##  9: 2021-10-05 12:04:29        9 &lt;ResampleResult[20]&gt;
## 10: 2021-10-05 12:04:29       10 &lt;ResampleResult[20]&gt;
## 11: 2021-10-05 12:04:29       11 &lt;ResampleResult[20]&gt;
## 12: 2021-10-05 12:04:29       12 &lt;ResampleResult[20]&gt;
## 13: 2021-10-05 12:04:29       13 &lt;ResampleResult[20]&gt;
## 14: 2021-10-05 12:04:29       14 &lt;ResampleResult[20]&gt;
## 15: 2021-10-05 12:04:30       15 &lt;ResampleResult[20]&gt;
## 16: 2021-10-05 12:04:30       16 &lt;ResampleResult[20]&gt;
## 17: 2021-10-05 12:04:30       17 &lt;ResampleResult[20]&gt;
## 18: 2021-10-05 12:04:30       18 &lt;ResampleResult[20]&gt;
## 19: 2021-10-05 12:04:30       19 &lt;ResampleResult[20]&gt;
## 20: 2021-10-05 12:04:30       20 &lt;ResampleResult[20]&gt;</code></pre>
<p>In sum, the grid search evaluated 20/25 different configurations of the grid in a random order before the <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminator</code></a> stopped the tuning.</p>
<p>The associated resampling iterations can be accessed in the <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a>:</p>
<div class="sourceCode" id="cb211"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">benchmark_result</span></code></pre></div>
<pre><code>## &lt;BenchmarkResult&gt; of 20 rows with 20 resampling runs
##  nr task_id    learner_id resampling_id iters warnings errors
##   1    pima classif.rpart       holdout     1        0      0
##   2    pima classif.rpart       holdout     1        0      0
##   3    pima classif.rpart       holdout     1        0      0
##   4    pima classif.rpart       holdout     1        0      0
##   5    pima classif.rpart       holdout     1        0      0
##   6    pima classif.rpart       holdout     1        0      0
##   7    pima classif.rpart       holdout     1        0      0
##   8    pima classif.rpart       holdout     1        0      0
##   9    pima classif.rpart       holdout     1        0      0
##  10    pima classif.rpart       holdout     1        0      0
##  11    pima classif.rpart       holdout     1        0      0
##  12    pima classif.rpart       holdout     1        0      0
##  13    pima classif.rpart       holdout     1        0      0
##  14    pima classif.rpart       holdout     1        0      0
##  15    pima classif.rpart       holdout     1        0      0
##  16    pima classif.rpart       holdout     1        0      0
##  17    pima classif.rpart       holdout     1        0      0
##  18    pima classif.rpart       holdout     1        0      0
##  19    pima classif.rpart       holdout     1        0      0
##  20    pima classif.rpart       holdout     1        0      0</code></pre>
<p>The <code>uhash</code> column links the resampling iterations to the evaluated configurations stored in <code>instance$archive$data</code>. This allows e.g. to score the included <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a>s on a different measure.</p>
<div class="sourceCode" id="cb213"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">benchmark_result</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##                                    uhash nr              task task_id
##  1: f190f139-ee9c-4bfa-a26a-c2355b86ae93  1 &lt;TaskClassif[47]&gt;    pima
##  2: 4df49a4d-e6cf-4b7d-8da4-3794d5b6a7f1  2 &lt;TaskClassif[47]&gt;    pima
##  3: 26d17200-7ddb-4f6f-bc4b-3ffa287b1d07  3 &lt;TaskClassif[47]&gt;    pima
##  4: bf98f46b-db87-4441-a7d4-c58cfcd4a179  4 &lt;TaskClassif[47]&gt;    pima
##  5: eb48c123-9eb3-4057-bd6a-c80a1a6265cb  5 &lt;TaskClassif[47]&gt;    pima
##  6: 09a84fcf-ab54-49fe-83b7-d89fe1b2754d  6 &lt;TaskClassif[47]&gt;    pima
##  7: 13332517-526b-464a-9c36-43780589a20c  7 &lt;TaskClassif[47]&gt;    pima
##  8: bdbcb5d8-3bb4-4adf-9969-bdafdd207412  8 &lt;TaskClassif[47]&gt;    pima
##  9: eabd9347-bedd-430c-ac72-b8187b39277b  9 &lt;TaskClassif[47]&gt;    pima
## 10: cb945391-2007-4c8c-875e-d37ba6b789e6 10 &lt;TaskClassif[47]&gt;    pima
## 11: ffd3d3e2-5550-4849-ab04-6e2834231846 11 &lt;TaskClassif[47]&gt;    pima
## 12: 78d49191-e8ef-4cb9-9ec2-9ae41abe791a 12 &lt;TaskClassif[47]&gt;    pima
## 13: 1f28e57b-54ed-4dc1-9f69-f8dec555dce2 13 &lt;TaskClassif[47]&gt;    pima
## 14: 3441f789-6dba-4d23-beef-7b861de27f18 14 &lt;TaskClassif[47]&gt;    pima
## 15: 971658d6-54db-41bd-8b18-f4de4ecd34ff 15 &lt;TaskClassif[47]&gt;    pima
## 16: 30ed6670-a958-466b-bd8e-1a59937bf965 16 &lt;TaskClassif[47]&gt;    pima
## 17: 4081e1cf-42d3-40a0-a3a2-5eafcbb94827 17 &lt;TaskClassif[47]&gt;    pima
## 18: 9bf5a2ca-1159-4c9f-8dc4-8653ccaa7c4e 18 &lt;TaskClassif[47]&gt;    pima
## 19: d1f72d10-0a45-45bf-83a9-1a9bdc339a67 19 &lt;TaskClassif[47]&gt;    pima
## 20: 43ab6e23-e230-4a47-99a3-76ad2a093eea 20 &lt;TaskClassif[47]&gt;    pima
##                       learner    learner_id              resampling
##  1: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  2: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  3: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  4: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  5: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  6: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  7: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  8: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##  9: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 10: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 11: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 12: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 13: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 14: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 15: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 16: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 17: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 18: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 19: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
## 20: &lt;LearnerClassifRpart[36]&gt; classif.rpart &lt;ResamplingHoldout[19]&gt;
##     resampling_id iteration              prediction classif.acc
##  1:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7422
##  2:       holdout         1 &lt;PredictionClassif[19]&gt;      0.6914
##  3:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
##  4:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
##  5:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7422
##  6:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7227
##  7:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
##  8:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
##  9:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 10:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 11:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 12:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 13:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 14:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 15:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7422
## 16:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7422
## 17:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 18:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109
## 19:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7422
## 20:       holdout         1 &lt;PredictionClassif[19]&gt;      0.7109</code></pre>
<p>Now the optimized hyperparameters can take the previously created <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a>, set the returned hyperparameters and train it on the full dataset.</p>
<div class="sourceCode" id="cb215"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">result_learner_param_vals</span>
<span class="va">learner</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></code></pre></div>
<p>The trained model can now be used to make a prediction on external data.
Note that predicting on observations present in the <code>task</code>, should be avoided.
The model has seen these observations already during tuning and therefore results would be statistically biased.
Hence, the resulting performance measure would be over-optimistic.
Instead, to get statistically unbiased performance estimates for the current task, <a href="optimization.html#nested-resampling">nested resampling</a> is required.</p>
</div>
<div id="autotuner" class="section level3" number="3.1.4">
<h3>
<span class="header-section-number">3.1.4</span> Automating the Tuning<a class="anchor" aria-label="anchor" href="#autotuner"><i class="fas fa-link"></i></a>
</h3>
<p>The <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> wraps a learner and augments it with an automatic tuning for a given set of hyperparameters.
Because the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> itself inherits from the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> base class, it can be used like any other learner.
Analogously to the previous subsection, a new classification tree learner is created.
This classification tree learner automatically tunes the parameters <code>cp</code> and <code>minsplit</code> using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:</p>
<div class="sourceCode" id="cb216"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span>
<span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cp <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0.001</span>, upper <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,
  minsplit <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_int</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="op">)</span>
<span class="va">terminator</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span>

<span class="va">at</span> <span class="op">=</span> <span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html">AutoTuner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  learner <span class="op">=</span> <span class="va">learner</span>,
  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,
  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,
  search_space <span class="op">=</span> <span class="va">search_space</span>,
  terminator <span class="op">=</span> <span class="va">terminator</span>,
  tuner <span class="op">=</span> <span class="va">tuner</span>
<span class="op">)</span>
<span class="va">at</span></code></pre></div>
<pre><code>## &lt;AutoTuner:classif.rpart.tuned&gt;
## * Model: -
## * Search Space:
## &lt;ParamSet&gt;
##          id    class lower upper nlevels        default value
## 1:       cp ParamDbl 0.001   0.1     Inf &lt;NoDefault[3]&gt;      
## 2: minsplit ParamInt 1.000  10.0      10 &lt;NoDefault[3]&gt;      
## * Packages: rpart
## * Predict Type: response
## * Feature Types: logical, integer, numeric, factor, ordered
## * Properties: importance, missings, multiclass, selected_features,
##   twoclass, weights</code></pre>
<p>We can now use the learner like any other learner, calling the <code>$train()</code> and <code>$predict()</code> method.</p>
<div class="sourceCode" id="cb218"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">at</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></code></pre></div>
<pre><code>## INFO  [12:04:31.140] [bbotk] Starting to optimize 2 parameter(s) with '&lt;OptimizerRandomSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=10, k=0]' 
## INFO  [12:04:31.161] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:31.274] [bbotk] Result of batch 1: 
## INFO  [12:04:31.275] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:31.275] [bbotk]  0.02492        4     0.2305             0.01 
## INFO  [12:04:31.275] [bbotk]                                 uhash 
## INFO  [12:04:31.275] [bbotk]  f221c690-1d59-480e-8d55-d728335b0509 
## INFO  [12:04:31.281] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:31.398] [bbotk] Result of batch 2: 
## INFO  [12:04:31.400] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:31.400] [bbotk]  0.05193        6     0.2539            0.011 
## INFO  [12:04:31.400] [bbotk]                                 uhash 
## INFO  [12:04:31.400] [bbotk]  2d343aef-5d95-41a2-93fe-6dacd88a0f2e 
## INFO  [12:04:31.405] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:31.519] [bbotk] Result of batch 3: 
## INFO  [12:04:31.521] [bbotk]      cp minsplit classif.ce runtime_learners 
## INFO  [12:04:31.521] [bbotk]  0.0369        4     0.2305            0.011 
## INFO  [12:04:31.521] [bbotk]                                 uhash 
## INFO  [12:04:31.521] [bbotk]  eea7c752-fe33-4493-beb1-9a27e8eb86c8 
## INFO  [12:04:31.525] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:31.634] [bbotk] Result of batch 4: 
## INFO  [12:04:31.642] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:31.642] [bbotk]  0.07791       10     0.2539            0.011 
## INFO  [12:04:31.642] [bbotk]                                 uhash 
## INFO  [12:04:31.642] [bbotk]  c27d8f71-6024-4035-8324-c2f98dfeccf9 
## INFO  [12:04:31.647] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:31.752] [bbotk] Result of batch 5: 
## INFO  [12:04:31.755] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:31.755] [bbotk]  0.07348        4     0.2539            0.013 
## INFO  [12:04:31.755] [bbotk]                                 uhash 
## INFO  [12:04:31.755] [bbotk]  9328f33b-056a-4edc-8d38-59424ab917f3 
## INFO  [12:04:31.761] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:31.891] [bbotk] Result of batch 6: 
## INFO  [12:04:31.893] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:31.893] [bbotk]  0.06864        3     0.2539            0.011 
## INFO  [12:04:31.893] [bbotk]                                 uhash 
## INFO  [12:04:31.893] [bbotk]  60fe583e-baf3-4cc0-bb83-01d6e843d5a5 
## INFO  [12:04:31.899] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:32.007] [bbotk] Result of batch 7: 
## INFO  [12:04:32.010] [bbotk]      cp minsplit classif.ce runtime_learners 
## INFO  [12:04:32.010] [bbotk]  0.0895       10     0.2539             0.01 
## INFO  [12:04:32.010] [bbotk]                                 uhash 
## INFO  [12:04:32.010] [bbotk]  0ff2ab8a-22fc-410b-9580-f2acafbbf342 
## INFO  [12:04:32.014] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:32.133] [bbotk] Result of batch 8: 
## INFO  [12:04:32.135] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:32.135] [bbotk]  0.04525        1     0.2305            0.011 
## INFO  [12:04:32.135] [bbotk]                                 uhash 
## INFO  [12:04:32.135] [bbotk]  9864a697-b477-4805-a777-4045ec47618f 
## INFO  [12:04:32.140] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:32.240] [bbotk] Result of batch 9: 
## INFO  [12:04:32.244] [bbotk]      cp minsplit classif.ce runtime_learners 
## INFO  [12:04:32.244] [bbotk]  0.0495        4     0.2305            0.009 
## INFO  [12:04:32.244] [bbotk]                                 uhash 
## INFO  [12:04:32.244] [bbotk]  44109589-3e65-47fe-8621-b63e45448dad 
## INFO  [12:04:32.249] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:32.369] [bbotk] Result of batch 10: 
## INFO  [12:04:32.371] [bbotk]       cp minsplit classif.ce runtime_learners 
## INFO  [12:04:32.371] [bbotk]  0.04341        5     0.2305            0.011 
## INFO  [12:04:32.371] [bbotk]                                 uhash 
## INFO  [12:04:32.371] [bbotk]  d55a139b-8b33-4613-b8a1-3d2a2524f38c 
## INFO  [12:04:32.380] [bbotk] Finished optimizing after 10 evaluation(s) 
## INFO  [12:04:32.381] [bbotk] Result: 
## INFO  [12:04:32.383] [bbotk]       cp minsplit learner_param_vals  x_domain classif.ce 
## INFO  [12:04:32.383] [bbotk]  0.02492        4          &lt;list[3]&gt; &lt;list[2]&gt;     0.2305</code></pre>
<p>We can also pass it to <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> and <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a>. This is called <a href="optimization.html#nested-resampling">nested resampling</a> which is discussed in the next chapter.</p>

</div>
</div>
<div id="searchspace" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Tuning Search Spaces<a class="anchor" aria-label="anchor" href="#searchspace"><i class="fas fa-link"></i></a>
</h2>
<p>When running an optimization, it is important to inform the tuning algorithm about what hyperparameters are valid.
Here the names, types, and valid ranges of each hyperparameter are important.
All this information is communicated with objects of the class <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>, which is defined in <a href="https://paradox.mlr-org.com">paradox</a>.
While it is possible to create <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>-objects using its <code>$new</code>-constructor, it is much shorter and readable to use the <a href="https://paradox.mlr-org.com/reference/ps.html"><code>ps</code></a>-shortcut, which will be presented here.
For an in-depth description of <a href="https://paradox.mlr-org.com">paradox</a> and its classes, see the <a href="technical.html#paradox"><code>paradox</code> chapter</a>.</p>
<p>Note, that <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> objects exist in two contexts.
First, <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>-objects are used to define the space of valid parameter setting for a learner (and other objects).
Second, they are used to define a search space for tuning.
We are mainly interested in the latter.
For an example we can consider the <code>minsplit</code> parameter of the <a href="https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html"><code>classif.rpart Learner</code></a>.
The <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> associated with the learner has a lower but <em>no</em> upper bound.
However, for tuning the value, a lower <em>and</em> upper bound must be given because tuning search spaces need to be bounded.
For <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> or <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html"><code>PipeOp</code></a> objects, typically “unbounded” <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSets</code></a> are used.
Here, however, we will mainly focus on creating “bounded” <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSets</code></a> that can be used for tuning.
See the <a href="technical.html#paradox">in-depth <code>paradox</code> chapter</a> for more details on using <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSets</code></a> to define parameter ranges for use-cases besides tuning.</p>
<div id="creating-paramsets" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Creating <code>ParamSet</code>s<a class="anchor" aria-label="anchor" href="#creating-paramsets"><i class="fas fa-link"></i></a>
</h3>
<p>An empty <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> – not yet very useful – can be constructed using just the <a href="https://paradox.mlr-org.com/reference/ps.html"><code>ps</code></a> call:</p>
<div class="sourceCode" id="cb220"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3verse.mlr-org.com">"mlr3verse"</a></span><span class="op">)</span>

<span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">search_space</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
## Empty.</code></pre>
<p><a href="https://paradox.mlr-org.com/reference/ps.html"><code>ps</code></a> takes named <a href="https://paradox.mlr-org.com/reference/Domain.html"><code>Domain</code></a> arguments that are turned into parameters. A possible search space for the <code>"classif.svm"</code> learner could for example be:</p>
<div class="sourceCode" id="cb222"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0.1</span>, upper <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">search_space</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##        id    class lower upper nlevels        default value
## 1:   cost ParamDbl   0.1    10     Inf &lt;NoDefault[3]&gt;      
## 2: kernel ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;</code></pre>
<p>There are five domain constructors that produce a parameters when given to <a href="https://paradox.mlr-org.com/reference/ps.html"><code>ps</code></a>:</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="21%">
<col width="31%">
<col width="29%">
<col width="16%">
</colgroup>
<thead><tr class="header">
<th align="center">Constructor</th>
<th align="center">Description</th>
<th align="center">Is bounded?</th>
<th align="center">Underlying Class</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><code>p_dbl</code></td>
<td align="center">Real valued parameter (“double”)</td>
<td align="center">When <code>upper</code> and <code>lower</code> are given</td>
<td align="center"><a href="https://paradox.mlr-org.com/reference/ParamDbl.html"><code>ParamDbl</code></a></td>
</tr>
<tr class="even">
<td align="center"><code>p_int</code></td>
<td align="center">Integer parameter</td>
<td align="center">When <code>upper</code> and <code>lower</code> are given</td>
<td align="center"><a href="https://paradox.mlr-org.com/reference/ParamInt.html"><code>ParamInt</code></a></td>
</tr>
<tr class="odd">
<td align="center"><code>p_fct</code></td>
<td align="center">Discrete valued parameter (“factor”)</td>
<td align="center">Always</td>
<td align="center"><a href="https://paradox.mlr-org.com/reference/ParamFct.html"><code>ParamFct</code></a></td>
</tr>
<tr class="even">
<td align="center"><code>p_lgl</code></td>
<td align="center">Logical / Boolean parameter</td>
<td align="center">Always</td>
<td align="center"><a href="https://paradox.mlr-org.com/reference/ParamLgl.html"><code>ParamLgl</code></a></td>
</tr>
<tr class="odd">
<td align="center"><code>p_uty</code></td>
<td align="center">Untyped parameter</td>
<td align="center">Never</td>
<td align="center"><a href="https://paradox.mlr-org.com/reference/ParamUty.html"><code>ParamUty</code></a></td>
</tr>
</tbody>
</table></div>
<p>These domain constructors each take some of the following arguments:</p>
<ul>
<li>
<strong><code>lower</code></strong>, <strong><code>upper</code></strong>: lower and upper bound of numerical parameters (<code>p_dbl</code> and <code>p_int</code>). These need to be given to get bounded parameter spaces valid for tuning.</li>
<li>
<strong><code>levels</code></strong>: Allowed categorical values for <code>p_fct</code> parameters. Required argument for <code>p_fct</code>. See <a href="optimization.html#autolevel">below</a> for more details on this parameter.</li>
<li>
<strong><code>trafo</code></strong>: transformation function, see <a href="optimization.html#searchspace-trafo">below</a>.</li>
<li>
<strong><code>depends</code></strong>: dependencies, see <a href="optimization.html#searchspace-depends">below</a>.</li>
<li>
<strong><code>tags</code></strong>: Further information about a parameter, used for example by the <a href="optimization.html#hyperband"><code>hyperband</code></a> tuner.</li>
<li>
<strong><code>default</code></strong>: Value corresponding to default behavior when the parameter is not given. Not used for tuning search spaces.</li>
<li>
<strong><code>special_vals</code></strong>: Valid values besides the normally accepted values for a parameter. Not used for tuning search spaces.</li>
<li>
<strong><code>custom_check</code></strong>: Function that checks whether a value given to <code>p_uty</code> is valid. Not used for tuning search spaces.</li>
</ul>
<p>The <code>lower</code>, <code>upper</code>, or <code>levels</code> parameters are always at the first (or second, for <code>upper</code>) position of the respective constructors, so it is preferred to omit them when defining a <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>, for improved conciseness:</p>
<div class="sourceCode" id="cb224"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">10</span><span class="op">)</span>, kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
</div>
<div id="searchspace-trafo" class="section level3" number="3.2.2">
<h3>
<span class="header-section-number">3.2.2</span> Transformations (<code>trafo</code>)<a class="anchor" aria-label="anchor" href="#searchspace-trafo"><i class="fas fa-link"></i></a>
</h3>
<p>We can use the <a href="https://paradox.mlr-org.com">paradox</a> function <a href="https://paradox.mlr-org.com/reference/generate_design_grid.html"><code>generate_design_grid</code></a> to look at the values that would be evaluated by grid search.
(We are using <code><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist()</a></code> here because the result of <code>$transpose()</code> is a list that is harder to read. If we didn’t use <code>$transpose()</code>, on the other hand, the transformations that we investigate here are not applied.)</p>
<div class="sourceCode" id="cb225"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://r-datatable.com">"data.table"</a></span><span class="op">)</span>
<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##     cost     kernel
## 1:  0.10 polynomial
## 2:  0.10     radial
## 3:  5.05 polynomial
## 4:  5.05     radial
## 5: 10.00 polynomial
## 6: 10.00     radial</code></pre>
<p>We notice that the <code>cost</code> parameter is taken on a linear scale.
We assume, however, that the difference of cost between <code>0.1</code> and <code>1</code> should have a similar effect as the difference between <code>1</code> and <code>10</code>.
Therefore it makes more sense to tune it on a <em>logarithmic scale</em>.
This is done by using a <strong>transformation</strong> (<code>trafo</code>).
This is a function that is applied to a parameter after it has been sampled by the tuner.
We can tune <code>cost</code> on a logarithmic scale by sampling on the linear scale <code>[-1, 1]</code> and computing <code>10^x</code> from that value.</p>
<div class="sourceCode" id="cb227"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">10</span><span class="op">^</span><span class="va">x</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    cost     kernel
## 1:  0.1 polynomial
## 2:  0.1     radial
## 3:  1.0 polynomial
## 4:  1.0     radial
## 5: 10.0 polynomial
## 6: 10.0     radial</code></pre>
<p>It is even possible to attach another transformation to the <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> as a whole that gets executed after individual parameter’s transformations were performed.
It is given through the <code>.extra_trafo</code> argument and should be a function with parameters <code>x</code> and <code>param_set</code> that takes a list of parameter values in <code>x</code> and returns a modified list.
This transformation can access all parameter values of an evaluation and modify them with interactions.
It is even possible to add or remove parameters.
(The following is a bit of a silly example.)</p>
<div class="sourceCode" id="cb229"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">10</span><span class="op">^</span><span class="va">x</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>,
  .extra_trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">param_set</span><span class="op">)</span> <span class="op">{</span>
    <span class="kw">if</span> <span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">kernel</span> <span class="op">==</span> <span class="st">"polynomial"</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">x</span><span class="op">$</span><span class="va">cost</span> <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">cost</span> <span class="op">*</span> <span class="fl">2</span>
    <span class="op">}</span>
    <span class="va">x</span>
  <span class="op">}</span>
<span class="op">)</span>
<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    cost     kernel
## 1:  0.2 polynomial
## 2:  0.1     radial
## 3:  2.0 polynomial
## 4:  1.0     radial
## 5: 20.0 polynomial
## 6: 10.0     radial</code></pre>
<p>The available types of search space parameters are limited: continuous, integer, discrete, and logical scalars.
There are many machine learning algorithms, however, that take parameters of other types, for example vectors or functions.
These can not be defined in a search space <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>, and they are often given as <a href="https://paradox.mlr-org.com/reference/ParamUty.html"><code>ParamUty</code></a> in the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a>’s <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>.
When trying to tune over these hyperparameters, it is necessary to perform a Transformation that changes the type of a parameter.</p>
<p>An example is the <code>class.weights</code> parameter of the SVM, which takes a named vector of class weights with one entry for each target class.
The trafo that would tune <code>class.weights</code> for the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html"><code>tsk("spam")</code></a> dataset could be:</p>
<div class="sourceCode" id="cb231"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  class.weights <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.9</span>, trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>spam <span class="op">=</span> <span class="va">x</span>, nonspam <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## [[1]]
## [[1]]$class.weights
##    spam nonspam 
##     0.1     0.9 
## 
## 
## [[2]]
## [[2]]$class.weights
##    spam nonspam 
##     0.5     0.5 
## 
## 
## [[3]]
## [[3]]$class.weights
##    spam nonspam 
##     0.9     0.1</code></pre>
<p>(We are omitting <code><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist()</a></code> in this example because it breaks the vector valued return elements.)</p>
</div>
<div id="autolevel" class="section level3" number="3.2.3">
<h3>
<span class="header-section-number">3.2.3</span> Automatic Factor Level Transformation<a class="anchor" aria-label="anchor" href="#autolevel"><i class="fas fa-link"></i></a>
</h3>
<p>A common use-case is the necessity to specify a list of values that should all be tried (or sampled from).
It may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried.
Or it may be that a choice of special numeric values should be tried.
For this, the <code>p_fct</code> constructor’s <code>level</code> argument may be a value that is not a <code>character</code> vector, but something else.
If, for example, only the values <code>0.1</code>, <code>3</code>, and <code>10</code> should be tried for the <code>cost</code> parameter, even when doing random search, then the following search space would achieve that:</p>
<div class="sourceCode" id="cb233"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    cost     kernel
## 1:  0.1 polynomial
## 2:  0.1     radial
## 3:  3.0 polynomial
## 4:  3.0     radial
## 5: 10.0 polynomial
## 6: 10.0     radial</code></pre>
<p>This is equivalent to the following:</p>
<div class="sourceCode" id="cb235"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"0.1"</span>, <span class="st">"3"</span>, <span class="st">"10"</span><span class="op">)</span>,
    trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>`0.1` <span class="op">=</span> <span class="fl">0.1</span>, `3` <span class="op">=</span> <span class="fl">3</span>, `10` <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">[[</span><span class="va">x</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    cost     kernel
## 1:  0.1 polynomial
## 2:  0.1     radial
## 3:  3.0 polynomial
## 4:  3.0     radial
## 5: 10.0 polynomial
## 6: 10.0     radial</code></pre>
<p>This may seem silly, but makes sense when considering that factorial tuning parameters are always <code>character</code> values:</p>
<div class="sourceCode" id="cb237"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/typeof.html">typeof</a></span><span class="op">(</span><span class="va">search_space</span><span class="op">$</span><span class="va">params</span><span class="op">$</span><span class="va">cost</span><span class="op">$</span><span class="va">levels</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "character"</code></pre>
<p>Be aware that this results in an “unordered” hyperparameter, however.
Tuning algorithms that make use of ordering information of parameters, like genetic algorithms or model based optimization, will perform worse when this is done.
For these algorithms, it may make more sense to define a <code>p_dbl</code> or <code>p_int</code> with a more fitting trafo.</p>
<p>The <code>class.weights</code> case from above can also be implemented like this, if there are only a few candidates of <code>class.weights</code> vectors that should be tried.
Note that the <code>levels</code> argument of <code>p_fct</code> must be named if there is no easy way for <code><a href="https://rdrr.io/r/base/character.html">as.character()</a></code> to create names:</p>
<div class="sourceCode" id="cb239"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  class.weights <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span>
    <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
      candidate_a <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>spam <span class="op">=</span> <span class="fl">0.5</span>, nonspam <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,
      candidate_b <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>spam <span class="op">=</span> <span class="fl">0.3</span>, nonspam <span class="op">=</span> <span class="fl">0.7</span><span class="op">)</span>
    <span class="op">)</span>
  <span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## [[1]]
## [[1]]$class.weights
##    spam nonspam 
##     0.5     0.5 
## 
## 
## [[2]]
## [[2]]$class.weights
##    spam nonspam 
##     0.3     0.7</code></pre>
</div>
<div id="searchspace-depends" class="section level3" number="3.2.4">
<h3>
<span class="header-section-number">3.2.4</span> Parameter Dependencies (<code>depends</code>)<a class="anchor" aria-label="anchor" href="#searchspace-depends"><i class="fas fa-link"></i></a>
</h3>
<p>Some parameters are only relevant when another parameter has a certain value, or one of several values.
The SVM, for example, has the <code>degree</code> parameter that is only valid when <code>kernel</code> is <code>"polynomial"</code>.
This can be specified using the <code>depends</code> argument.
It is an expression that must involve other parameters and be of the form <code>&lt;param&gt; == &lt;scalar&gt;</code>, <code>&lt;param&gt; %in% &lt;vector&gt;</code>, or multiple of these chained by <code><a href="https://rdrr.io/r/base/Logic.html">&amp;&amp;</a></code>.
To tune the <code>degree</code> parameter, one would need to do the following:</p>
<div class="sourceCode" id="cb241"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  cost <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">10</span><span class="op">^</span><span class="va">x</span><span class="op">)</span>,
  kernel <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>,
  degree <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_int</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span>, depends <span class="op">=</span> <span class="va">kernel</span> <span class="op">==</span> <span class="st">"polynomial"</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">search_space</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span>, fill <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<pre><code>##     cost     kernel degree
##  1:  0.1 polynomial      1
##  2:  0.1 polynomial      2
##  3:  0.1 polynomial      3
##  4:  0.1     radial     NA
##  5:  1.0 polynomial      1
##  6:  1.0 polynomial      2
##  7:  1.0 polynomial      3
##  8:  1.0     radial     NA
##  9: 10.0 polynomial      1
## 10: 10.0 polynomial      2
## 11: 10.0 polynomial      3
## 12: 10.0     radial     NA</code></pre>
</div>
<div id="creating-tuning-paramsets-from-other-paramsets" class="section level3" number="3.2.5">
<h3>
<span class="header-section-number">3.2.5</span> Creating Tuning ParamSets from other ParamSets<a class="anchor" aria-label="anchor" href="#creating-tuning-paramsets-from-other-paramsets"><i class="fas fa-link"></i></a>
</h3>
<p>Having to define a tuning <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> for a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> that already has parameter set information may seem unnecessarily tedious, and there is indeed a way to create tuning <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSets</code></a> from a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a>’s <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>, making use of as much information as already available.</p>
<p>This is done by setting values of a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a>’s <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> to so-called <a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>TuneToken</code></a>s, constructed with a <a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>to_tune</code></a> call.
This can be done in the same way that other hyperparameters are set to specific values.
It can be understood as the hyperparameters being tagged for later tuning.
The resulting <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> used for tuning can be retrieved using the <code>$search_space()</code> method.</p>
<div class="sourceCode" id="cb243"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.svm"</span><span class="op">)</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">kernel</span> <span class="op">=</span> <span class="st">"polynomial"</span>  <span class="co"># for example</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">degree</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##        id    class lower upper nlevels        default value
## 1: degree ParamInt     1     3       3 &lt;NoDefault[3]&gt;</code></pre>
<div class="sourceCode" id="cb245"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    degree
## 1:      1
## 2:      2
## 3:      3</code></pre>
<p>It is possible to omit <code>lower</code> here, because it can be inferred from the lower bound of the <code>degree</code> parameter itself.
For other parameters, that are already bounded, it is possible to not give any bounds at all, because their ranges are already bounded.
An example is the logical <code>shrinking</code> hyperparameter:</p>
<div class="sourceCode" id="cb247"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">shrinking</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##           id    class lower upper nlevels        default value
## 1:    degree ParamInt     1     3       3 &lt;NoDefault[3]&gt;      
## 2: shrinking ParamLgl    NA    NA       2           TRUE</code></pre>
<div class="sourceCode" id="cb249"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    degree shrinking
## 1:      1      TRUE
## 2:      1     FALSE
## 3:      2      TRUE
## 4:      2     FALSE
## 5:      3      TRUE
## 6:      3     FALSE</code></pre>
<p><a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>to_tune</code></a> can also be constructed with a <a href="https://paradox.mlr-org.com/reference/Domain.html"><code>Domain</code></a> object, i.e. something constructed with a <code>p_***</code> call.
This way it is possible to tune continuous parameters with discrete values, or to give trafos or dependencies.
One could, for example, tune the <code>cost</code> as above on three given special values, and introduce a dependency of <code>shrinking</code> on it.
Notice that a short form for <code>to_tune(&lt;levels&gt;)</code> is a short form of <code>to_tune(p_fct(&lt;levels&gt;))</code>.
(When introducing the dependency, we need to use the <code>degree</code> value from <em>before</em> the implicit trafo, which is the name or <code><a href="https://rdrr.io/r/base/character.html">as.character()</a></code> of the respective value, here <code>"val2"</code>!)</p>
<div class="sourceCode" id="cb251"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">type</span> <span class="op">=</span> <span class="st">"C-classification"</span>  <span class="co"># needs to be set because of a bug in paradox</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">cost</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>val1 <span class="op">=</span> <span class="fl">0.3</span>, val2 <span class="op">=</span> <span class="fl">0.7</span><span class="op">)</span><span class="op">)</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">shrinking</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_lgl</a></span><span class="op">(</span>depends <span class="op">=</span> <span class="va">cost</span> <span class="op">==</span> <span class="st">"val2"</span><span class="op">)</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##           id    class lower upper nlevels        default parents value
## 1:      cost ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;              
## 2:    degree ParamInt     1     3       3 &lt;NoDefault[3]&gt;              
## 3: shrinking ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;    cost      
## Trafo is set.</code></pre>
<div class="sourceCode" id="cb253"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span>, fill <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<pre><code>##    degree cost shrinking
## 1:      1  0.3        NA
## 2:      1  0.7      TRUE
## 3:      1  0.7     FALSE
## 4:      2  0.3        NA
## 5:      2  0.7      TRUE
## 6:      2  0.7     FALSE
## 7:      3  0.3        NA
## 8:      3  0.7      TRUE
## 9:      3  0.7     FALSE</code></pre>
<p>The <code>search_space()</code> picks up dependencies fromt the underlying <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> automatically.
So if the <code>kernel</code> is tuned, then <code>degree</code> automatically gets the dependency on it, without us having to specify that.
(Here we reset <code>cost</code> and <code>shrinking</code> to <code>NULL</code> for the sake of clarity of the generated output.)</p>
<div class="sourceCode" id="cb255"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">cost</span> <span class="op">=</span> <span class="cn">NULL</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">shrinking</span> <span class="op">=</span> <span class="cn">NULL</span>
<span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">kernel</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##        id    class lower upper nlevels        default parents value
## 1: degree ParamInt     1     3       3 &lt;NoDefault[3]&gt;  kernel      
## 2: kernel ParamFct    NA    NA       2 &lt;NoDefault[3]&gt;</code></pre>
<div class="sourceCode" id="cb257"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbindlist</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span>, fill <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<pre><code>##        kernel degree
## 1: polynomial      1
## 2: polynomial      2
## 3: polynomial      3
## 4:     radial     NA</code></pre>
<p>It is even possible to define whole <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a>s that get tuned over for a single parameter.
This may be especially useful for vector hyperparameters that should be searched along multiple dimensions.
This <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> must, however, have an <code>.extra_trafo</code> that returns a list with a single element, because it corresponds to a single hyperparameter that is being tuned.
Suppose the <code>class.weights</code> hyperparameter should be tuned along two dimensions:</p>
<div class="sourceCode" id="cb259"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">class.weights</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span>
  <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>spam <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.9</span><span class="op">)</span>, nonspam <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.9</span><span class="op">)</span>,
    .extra_trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">param_set</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>spam <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">spam</span>, nonspam <span class="op">=</span> <span class="va">x</span><span class="op">$</span><span class="va">nonspam</span><span class="op">)</span><span class="op">)</span>
  <span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">search_space</span><span class="op">(</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="fu">transpose</span><span class="op">(</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span></code></pre></div>
<pre><code>## [[1]]
## [[1]]$kernel
## [1] "polynomial"
## 
## [[1]]$degree
## [1] 1
## 
## [[1]]$class.weights
##    spam nonspam 
##     0.1     0.1 
## 
## 
## [[2]]
## [[2]]$kernel
## [1] "polynomial"
## 
## [[2]]$degree
## [1] 1
## 
## [[2]]$class.weights
##    spam nonspam 
##     0.1     0.5 
## 
## 
## [[3]]
## [[3]]$kernel
## [1] "polynomial"
## 
## [[3]]$degree
## [1] 1
## 
## [[3]]$class.weights
##    spam nonspam 
##     0.1     0.9</code></pre>

</div>
</div>
<div id="nested-resampling" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Nested Resampling<a class="anchor" aria-label="anchor" href="#nested-resampling"><i class="fas fa-link"></i></a>
</h2>
<p>Evaluating a machine learning model often requires an additional layer of resampling when hyperparameters or features have to be selected.
Nested resampling separates these model selection steps from the process estimating the performance of the model.
If the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.
One reason is that the repeated evaluation of the model on the test data could leak information about its structure into the model, what results in over-optimistic performance estimates.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should be not used to construct a final model <span class="citation">(<a href="references.html#ref-Simon2007" role="doc-biblioref">Simon 2007</a>)</span>.
<img src="images/nested_resampling.png" width="98%" style="display: block; margin: auto;"></p>
<p>The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop.</p>
<p>In the outer resampling loop, we have three pairs of training/test sets.
On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop.
This way, we get one set of selected hyperparameters for each outer training set.
Then the learner is fitted on each outer training set using the corresponding selected hyperparameters.
Subsequently, we can evaluate the performance of the learner on the outer test sets.
The aggregated performance on the outer test sets is the unbiased performance estimate of the model.</p>
<div id="nested-resamp-exec" class="section level3" number="3.3.1">
<h3>
<span class="header-section-number">3.3.1</span> Execution<a class="anchor" aria-label="anchor" href="#nested-resamp-exec"><i class="fas fa-link"></i></a>
</h3>
<p>The previous <a href="optimization.html#tuning">section</a> examined the optimization of a simple classification tree on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html"><code>mlr_tasks_pima</code></a>.
We continue the example and estimate the predictive performance of the model with nested resampling.</p>
<p>We use a 4-fold cross-validation in the inner resampling loop.
The <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by grid search.</p>
<div class="sourceCode" id="cb261"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3verse.mlr-org.com">"mlr3verse"</a></span><span class="op">)</span>

<span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span>
<span class="va">resampling</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>
<span class="va">measure</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>
<span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0.001</span>, upper <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span>
<span class="va">terminator</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>
<span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"grid_search"</span>, resolution <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>

<span class="va">at</span> <span class="op">=</span> <span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html">AutoTuner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">learner</span>, <span class="va">resampling</span>, <span class="va">measure</span>, <span class="va">terminator</span>, <span class="va">tuner</span>, <span class="va">search_space</span><span class="op">)</span></code></pre></div>
<p>A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> to the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> function.
We have to set <code>store_models = TRUE</code> because we need the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> models to investigate the inner tuning.</p>
<div class="sourceCode" id="cb262"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"pima"</span><span class="op">)</span>
<span class="va">outer_resampling</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>

<span class="va">rr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/resample.html">resample</a></span><span class="op">(</span><span class="va">task</span>, <span class="va">at</span>, <span class="va">outer_resampling</span>, store_models <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<pre><code>## INFO  [12:04:42.164] [bbotk] Starting to optimize 1 parameter(s) with '&lt;OptimizerGridSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=5, k=0]' 
## INFO  [12:04:42.207] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:42.339] [bbotk] Result of batch 1: 
## INFO  [12:04:42.342] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:42.342] [bbotk]  0.001     0.2398            0.016 ff3bc9a1-eb4b-4e2f-8bad-ce6aac966424 
## INFO  [12:04:42.344] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:42.448] [bbotk] Result of batch 2: 
## INFO  [12:04:42.450] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:42.450] [bbotk]  0.012     0.2339            0.009 733b0f27-afe6-4d90-922e-a10522412284 
## INFO  [12:04:42.452] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:42.561] [bbotk] Result of batch 3: 
## INFO  [12:04:42.563] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:42.563] [bbotk]  0.045     0.2398            0.008 1ff9ef52-ed31-4ace-bc11-3ff53689b5df 
## INFO  [12:04:42.565] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:42.677] [bbotk] Result of batch 4: 
## INFO  [12:04:42.680] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:42.680] [bbotk]  0.078     0.2398            0.011 2677d9a2-90df-4445-9e70-f76faba4de2a 
## INFO  [12:04:42.681] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:42.794] [bbotk] Result of batch 5: 
## INFO  [12:04:42.796] [bbotk]   cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:42.796] [bbotk]  0.1     0.2398            0.016 6e8b9142-9bac-41f1-8768-88ad92731fe7 
## INFO  [12:04:42.803] [bbotk] Finished optimizing after 5 evaluation(s) 
## INFO  [12:04:42.804] [bbotk] Result: 
## INFO  [12:04:42.806] [bbotk]     cp learner_param_vals  x_domain classif.ce 
## INFO  [12:04:42.806] [bbotk]  0.012          &lt;list[2]&gt; &lt;list[1]&gt;     0.2339 
## INFO  [12:04:42.872] [bbotk] Starting to optimize 1 parameter(s) with '&lt;OptimizerGridSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=5, k=0]' 
## INFO  [12:04:42.875] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:42.975] [bbotk] Result of batch 1: 
## INFO  [12:04:42.977] [bbotk]   cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:42.977] [bbotk]  0.1     0.3099            0.009 ac8dd562-bd22-4e69-a5cf-fa0f0b5472d1 
## INFO  [12:04:42.979] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.086] [bbotk] Result of batch 2: 
## INFO  [12:04:43.088] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.088] [bbotk]  0.056     0.2515            0.009 d9f8ec39-2e8f-4f46-9486-5be63ce4febd 
## INFO  [12:04:43.089] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.278] [bbotk] Result of batch 3: 
## INFO  [12:04:43.280] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.280] [bbotk]  0.001     0.3392            0.012 f80091e5-8e87-492b-8973-852f3a699598 
## INFO  [12:04:43.282] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.381] [bbotk] Result of batch 4: 
## INFO  [12:04:43.383] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.383] [bbotk]  0.067     0.3099            0.008 fadbf356-c2d0-471d-a122-58565b9eb944 
## INFO  [12:04:43.385] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.493] [bbotk] Result of batch 5: 
## INFO  [12:04:43.495] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.495] [bbotk]  0.045     0.2515             0.01 5b2674c4-cdd4-4515-a2f5-7aae307ee671 
## INFO  [12:04:43.500] [bbotk] Finished optimizing after 5 evaluation(s) 
## INFO  [12:04:43.501] [bbotk] Result: 
## INFO  [12:04:43.503] [bbotk]     cp learner_param_vals  x_domain classif.ce 
## INFO  [12:04:43.503] [bbotk]  0.056          &lt;list[2]&gt; &lt;list[1]&gt;     0.2515 
## INFO  [12:04:43.566] [bbotk] Starting to optimize 1 parameter(s) with '&lt;OptimizerGridSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=5, k=0]' 
## INFO  [12:04:43.570] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.670] [bbotk] Result of batch 1: 
## INFO  [12:04:43.672] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.672] [bbotk]  0.067     0.2339            0.011 48a4f254-18f9-4dbd-80bf-9d6ce1121058 
## INFO  [12:04:43.674] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.782] [bbotk] Result of batch 2: 
## INFO  [12:04:43.784] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.784] [bbotk]  0.045     0.2339            0.009 fb25029e-418f-4dcc-862e-4fa8e4e51d6e 
## INFO  [12:04:43.786] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:43.889] [bbotk] Result of batch 3: 
## INFO  [12:04:43.890] [bbotk]   cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:43.890] [bbotk]  0.1     0.3333             0.01 f4c3b347-7a3a-46c5-b37d-1ba4d38d6fa2 
## INFO  [12:04:43.892] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:44.006] [bbotk] Result of batch 4: 
## INFO  [12:04:44.009] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:44.009] [bbotk]  0.089     0.3333             0.01 6d5ac764-4d41-4033-bf28-56b249d7ad6d 
## INFO  [12:04:44.010] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:44.121] [bbotk] Result of batch 5: 
## INFO  [12:04:44.123] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:44.123] [bbotk]  0.012     0.2047            0.011 44fc3dcf-d094-42aa-bab2-99c6ad0c020f 
## INFO  [12:04:44.129] [bbotk] Finished optimizing after 5 evaluation(s) 
## INFO  [12:04:44.130] [bbotk] Result: 
## INFO  [12:04:44.131] [bbotk]     cp learner_param_vals  x_domain classif.ce 
## INFO  [12:04:44.131] [bbotk]  0.012          &lt;list[2]&gt; &lt;list[1]&gt;     0.2047</code></pre>
<p>You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> for a <a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a> and estimate the performance of a model which is fitted on an optimized feature subset.</p>
</div>
<div id="nested-resamp-eval" class="section level3" number="3.3.2">
<h3>
<span class="header-section-number">3.3.2</span> Evaluation<a class="anchor" aria-label="anchor" href="#nested-resamp-eval"><i class="fas fa-link"></i></a>
</h3>
<p>With the created <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> we can now inspect the executed resampling iterations more closely.
See the section on <a href="basics.html#resampling">Resampling</a> for more detailed information about <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> objects.</p>
<p>We check the inner tuning results for stable hyperparameters.
This means that the selected hyperparameters should not vary too much.
We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduces too much randomness.
Usually, we aim for the selection of stable hyperparameters for all outer training sets.</p>
<div class="sourceCode" id="cb264"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_results.html">extract_inner_tuning_results</a></span><span class="op">(</span><span class="va">rr</span><span class="op">)</span></code></pre></div>
<pre><code>##    iteration    cp classif.ce learner_param_vals  x_domain task_id
## 1:         1 0.056     0.2515          &lt;list[2]&gt; &lt;list[1]&gt;    pima
## 2:         2 0.012     0.2339          &lt;list[2]&gt; &lt;list[1]&gt;    pima
## 3:         3 0.012     0.2047          &lt;list[2]&gt; &lt;list[1]&gt;    pima
##             learner_id resampling_id
## 1: classif.rpart.tuned            cv
## 2: classif.rpart.tuned            cv
## 3: classif.rpart.tuned            cv</code></pre>
<p>Next, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.</p>
<div class="sourceCode" id="cb266"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rr</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>##                 task task_id         learner          learner_id
## 1: &lt;TaskClassif[47]&gt;    pima &lt;AutoTuner[40]&gt; classif.rpart.tuned
## 2: &lt;TaskClassif[47]&gt;    pima &lt;AutoTuner[40]&gt; classif.rpart.tuned
## 3: &lt;TaskClassif[47]&gt;    pima &lt;AutoTuner[40]&gt; classif.rpart.tuned
##            resampling resampling_id iteration              prediction
## 1: &lt;ResamplingCV[19]&gt;            cv         1 &lt;PredictionClassif[19]&gt;
## 2: &lt;ResamplingCV[19]&gt;            cv         2 &lt;PredictionClassif[19]&gt;
## 3: &lt;ResamplingCV[19]&gt;            cv         3 &lt;PredictionClassif[19]&gt;
##    classif.ce
## 1:     0.2109
## 2:     0.2812
## 3:     0.2266</code></pre>
<p>The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameter found by grid search.</p>
<div class="sourceCode" id="cb268"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## classif.ce 
##     0.2396</code></pre>
<p>Note that nested resampling is computationally expensive.
For this reason we use relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on <a href="technical.html#parallelization">Parallelization</a>.</p>
</div>
<div id="nested-final-model" class="section level3" number="3.3.3">
<h3>
<span class="header-section-number">3.3.3</span> Final Model<a class="anchor" aria-label="anchor" href="#nested-final-model"><i class="fas fa-link"></i></a>
</h3>
<p>We can use the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> to tune the hyperparameters of our learner and fit the final model on the full data set.</p>
<div class="sourceCode" id="cb270"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">at</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></code></pre></div>
<pre><code>## INFO  [12:04:44.494] [bbotk] Starting to optimize 1 parameter(s) with '&lt;OptimizerGridSearch&gt;' and '&lt;TerminatorEvals&gt; [n_evals=5, k=0]' 
## INFO  [12:04:44.497] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:44.610] [bbotk] Result of batch 1: 
## INFO  [12:04:44.612] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:44.612] [bbotk]  0.067     0.2539             0.01 8e2b0200-df76-4eb9-9496-10805a6d84db 
## INFO  [12:04:44.614] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:44.722] [bbotk] Result of batch 2: 
## INFO  [12:04:44.724] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:44.724] [bbotk]  0.078     0.2539            0.012 150723f3-37d1-4526-8cde-5f7e701ff3c0 
## INFO  [12:04:44.726] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:44.838] [bbotk] Result of batch 3: 
## INFO  [12:04:44.840] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:44.840] [bbotk]  0.012     0.2695            0.019 fcc33157-839e-4c92-8ce0-9b23b42e928f 
## INFO  [12:04:44.842] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:44.947] [bbotk] Result of batch 4: 
## INFO  [12:04:44.949] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:44.949] [bbotk]  0.034     0.2734             0.01 2de2982e-2fab-4fd1-bef8-8931b066c319 
## INFO  [12:04:44.951] [bbotk] Evaluating 1 configuration(s) 
## INFO  [12:04:45.060] [bbotk] Result of batch 5: 
## INFO  [12:04:45.062] [bbotk]     cp classif.ce runtime_learners                                uhash 
## INFO  [12:04:45.062] [bbotk]  0.001     0.2656            0.017 2559605d-9da6-435c-92fc-86d8dc438304 
## INFO  [12:04:45.069] [bbotk] Finished optimizing after 5 evaluation(s) 
## INFO  [12:04:45.070] [bbotk] Result: 
## INFO  [12:04:45.071] [bbotk]     cp learner_param_vals  x_domain classif.ce 
## INFO  [12:04:45.071] [bbotk]  0.067          &lt;list[2]&gt; &lt;list[1]&gt;     0.2539</code></pre>
<p>The trained model can now be used to make predictions on new data.
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (<code>at$tuning_result$classif.ce</code>) as the model’s performance.
Instead, we report the performance estimated with nested resampling as the performance of the model.</p>

</div>
</div>
<div id="hyperband" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Tuning with Hyperband<a class="anchor" aria-label="anchor" href="#hyperband"><i class="fas fa-link"></i></a>
</h2>
<p>Besides the more traditional tuning methods, the ecosystem around <a href="https://mlr3.mlr-org.com">mlr3</a> offers another procedure for hyperparameter optimization called Hyperband implemented in the <a href="https://mlr3hyperband.mlr-org.com">mlr3hyperband</a> package.</p>
<p>Hyperband is a budget-oriented procedure, weeding out suboptimal performing configurations early on during a partially sequential training process, increasing tuning efficiency as a consequence.
For this, a combination of incremental resource allocation and early stopping is used: As optimization progresses, computational resources are increased for more promising configurations, while less promising ones are terminated early.</p>
<p>To give an introductory analogy, imagine two horse trainers are given eight untrained horses.
Both trainers want to win the upcoming race, but they are only given 32 units of food.
Given that each horse can be fed up to 8 units food (“maximum budget” per horse), there is not enough food for all the horses.
It is critical to identify the most promising horses early, and give them enough food to improve.
So, the trainers need to develop a strategy to split up the food in the best possible way.
The first trainer is very optimistic and wants to explore the full capabilities of a horse, because he does not want to pass a judgment on a horse’s performance unless it has been fully trained.
So, he divides his budget by the maximum amount he can give to a horse (lets say eight, so <span class="math inline">\(32 / 8 = 4\)</span>) and randomly picks four horses - his budget simply is not enough to fully train more.
Those four horses are then trained to their full capabilities, while the rest is set free.
This way, the trainer is confident about choosing the best out of the four trained horses, but he might have overlooked the horse with the highest potential since he only focused on half of them.
The other trainer is more creative and develops a different strategy.
He thinks, if a horse is not performing well at the beginning, it will also not improve after further training.
Based on this assumption, he decides to give one unit of food to each horse and observes how they develop.
After the initial food is consumed, he checks their performance and kicks the slowest half out of his training regime.
Then, he increases the available food for the remaining, further trains them until the food is consumed again, only to kick out the worst half once more.
He repeats this until the one remaining horse gets the rest of the food.
This means only one horse is fully trained, but on the flip side, he was able to start training with all eight horses.</p>
<p>On race day, all the horses are put on the starting line.
But which trainer will have the winning horse?
The one, who tried to train a maximum amount of horses to their fullest?
Or the other one, who made assumptions about the training progress of his horses?
How the training phases may possibly look like is visualized in figure <a href="optimization.html#fig:03-optimization-hyperband-001">3.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:03-optimization-hyperband-001"></span>
<img src="images/horse_training1.png" alt="Visulization of how the training processes may look like. The left plot corresponds to the non-selective trainer, while the right one to the selective trainer." width="99%"><p class="caption">
Figure 3.1: Visulization of how the training processes may look like. The left plot corresponds to the non-selective trainer, while the right one to the selective trainer.
</p>
</div>
<p>Hyperband works very similar in some ways, but also different in others.
It is not embodied by one of the trainers in our analogy, but more by the person, who would pay them.
Hyperband consists of several brackets, each bracket corresponding to a trainer, and we do not care about horses but about hyperparameter configurations of a machine learning algorithm.
The budget is not in terms of food, but in terms of a hyperparameter of the learner that scales in some way with the computational effort.
An example is the number of epochs we train a neural network, or the number of iterations in boosting.
Furthermore, there are not only two brackets (or trainers), but several, each placed at a unique spot between fully explorative of later training stages and extremely selective, equal to higher exploration of early training stages.
The level of selection aggressiveness is handled by a user-defined parameter called <span class="math inline">\(\eta\)</span>.
So, <span class="math inline">\(1/\eta\)</span> is the fraction of remaining configurations after a bracket removes his worst performing ones, but <span class="math inline">\(\eta\)</span> is also the factor by that the budget is increased for the next stage.
Because there is a different maximum budget per configuration that makes sense in different scenarios, the user also has to set this as the <span class="math inline">\(R\)</span> parameter.
No further parameters are required for Hyperband – the full required budget across all brackets is indirectly given by
<span class="math display">\[ (\lfloor \log_{\eta}{R} \rfloor + 1)^2 * R\]</span> <span class="citation">(<a href="references.html#ref-Li2016" role="doc-biblioref">Li et al. 2016</a>)</span>.
To give an idea how a full bracket layout might look like for a specific <span class="math inline">\(R\)</span> and <span class="math inline">\(\eta\)</span>, a quick overview is given in the following table.</p>
<div class="inline-table"><table class="kable_wrapper">
<caption>
<span id="tab:03-optimization-hyperband-002">Table 3.1: </span>Hyperband layout for <span class="math inline">\(\eta = 2\)</span> and <span class="math inline">\(R = 8\)</span>, consisting of four brackets with <span class="math inline">\(n\)</span> as the amount of active configurations.
</caption>
<tbody><tr>
<td>
<table class="table table-sm">
<thead><tr class="header">
<th align="right">stage</th>
<th align="right">budget</th>
<th align="right">n</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">2</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">8</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</td>
<td>
<table class="table table-sm">
<thead><tr class="header">
<th align="right">stage</th>
<th align="right">budget</th>
<th align="right">n</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">2</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">4</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">8</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</td>
<td>
<table class="table table-sm">
<thead><tr class="header">
<th align="right">stage</th>
<th align="right">budget</th>
<th align="right">n</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">8</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
</td>
<td>
<table class="table table-sm">
<thead><tr class="header">
<th align="right">stage</th>
<th align="right">budget</th>
<th align="right">n</th>
</tr></thead>
<tbody><tr class="odd">
<td align="right">1</td>
<td align="right">8</td>
<td align="right">4</td>
</tr></tbody>
</table>
</td>
</tr></tbody>
</table></div>
<p>Of course, early termination based on a performance criterion may be disadvantageous if it is done too aggressively in certain scenarios.
A learner to jumping radically in its estimated performance during the training phase may get the best configurations canceled too early, simply because they do not improve quickly enough compared to others.
In other words, it is often unclear beforehand if having an high amount of configurations <span class="math inline">\(n\)</span>, that gets aggressively discarded early, is better than having a high budget <span class="math inline">\(B\)</span> per configuration.
The arising tradeoff, that has to be made, is called the “<span class="math inline">\(n\)</span> versus <span class="math inline">\(B/n\)</span> problem”.
To create a balance between selection based on early training performance versus exploration of training performances in later training stages, <span class="math inline">\(\lfloor \log_{\eta}{R} \rfloor + 1\)</span> brackets are constructed with an associated set of varying sized configurations.
Thus, some brackets contain more configurations, with a small initial budget.
In these, a lot are discarded after having been trained for only a short amount of time, corresponding to the selective trainer in our horse analogy.
Others are constructed with fewer configurations, where discarding only takes place after a significant amount of budget was consumed.
The last bracket usually never discards anything, but also starts with only very few configurations – this is equivalent to the trainer explorative of later stages.
The former corresponds high <span class="math inline">\(n\)</span>, while the latter high <span class="math inline">\(B/n\)</span>.
Even though different brackets are initialized with a different amount of configurations and different initial budget sizes, each bracket is assigned (approximately) the same budget <span class="math inline">\((\lfloor \log_{\eta}{R} \rfloor + 1) * R\)</span>.</p>
<p>The configurations at the start of each bracket are initialized by random, often uniform sampling.
Note that currently all configurations are trained completely from the beginning, so no online updates of models from stage to stage is happening.</p>
<p>To identify the budget for evaluating Hyperband, the user has to specify explicitly which hyperparameter of the learner influences the budget by extending a single hyperparameter in the <a href="https://paradox.mlr-org.com/reference/ParamSet.html"><code>ParamSet</code></a> with an argument (<code>tags = "budget"</code>), like in the following snippet:</p>
<div class="sourceCode" id="cb272"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3verse.mlr-org.com">"mlr3verse"</a></span><span class="op">)</span>

<span class="co"># Hyperparameter subset of XGBoost</span>
<span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  nrounds <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_int</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fl">16</span>, tags <span class="op">=</span> <span class="st">"budget"</span><span class="op">)</span>,
  booster <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gbtree"</span>, <span class="st">"gblinear"</span>, <span class="st">"dart"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>Thanks to the broad ecosystem of the <a href="https://mlr3verse.mlr-org.com">mlr3verse</a> a learner does not require a natural budget parameter.
A typical case of this would be decision trees.
By using subsampling as preprocessing with <a href="https://mlr3pipelines.mlr-org.com">mlr3pipelines</a>, we can work around a lacking budget parameter.</p>
<div class="sourceCode" id="cb273"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>

<span class="co"># extend "classif.rpart" with "subsampling" as preprocessing step</span>
<span class="va">ll</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3pipelines.mlr-org.com//reference/po.html">po</a></span><span class="op">(</span><span class="st">"subsample"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span>

<span class="co"># extend hyperparameters of "classif.rpart" with subsampling fraction as budget</span>
<span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  classif.rpart.cp <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0.001</span>, upper <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,
  classif.rpart.minsplit <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_int</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,
  subsample.frac <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0.1</span>, upper <span class="op">=</span> <span class="fl">1</span>, tags <span class="op">=</span> <span class="st">"budget"</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>We can now plug the new learner with the extended hyperparameter set into a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> the same way as usual.
Naturally, Hyperband terminates once all of its brackets are evaluated, so a <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminator</code></a> in the tuning instance acts as an upper bound and should be only set to a low value if one is unsure of how long Hyperband will take to finish under the given settings.</p>
<div class="sourceCode" id="cb274"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html">TuningInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  task <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span>,
  learner <span class="op">=</span> <span class="va">ll</span>,
  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,
  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,
  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"none"</span><span class="op">)</span>, <span class="co"># hyperband terminates itself</span>
  search_space <span class="op">=</span> <span class="va">search_space</span>
<span class="op">)</span></code></pre></div>
<p>Now, we initialize a new instance of the <a href="https://www.rdocumentation.org/packages/mlr3hyperband/topics/mlr_tuners_hyperband"><code>mlr3hyperband::mlr_tuners_hyperband</code></a> class and start tuning with it.</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3hyperband.mlr-org.com">"mlr3hyperband"</a></span><span class="op">)</span></code></pre></div>
<pre><code>## Loading required package: mlr3tuning</code></pre>
<pre><code>## Loading required package: paradox</code></pre>
<div class="sourceCode" id="cb278"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"hyperband"</span>, eta <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>

<span class="co"># reduce logging output</span>
<span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"bbotk"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"warn"</span><span class="op">)</span>

<span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></code></pre></div>
<pre><code>##    classif.rpart.cp classif.rpart.minsplit subsample.frac learner_param_vals
## 1:          0.07348                      5         0.1111          &lt;list[6]&gt;
##     x_domain classif.ce
## 1: &lt;list[3]&gt;       0.02</code></pre>
<p>To receive the results of each sampled configuration, we simply run the following snippet.</p>
<div class="sourceCode" id="cb280"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>
  <span class="st">"subsample.frac"</span>,
  <span class="st">"classif.rpart.cp"</span>,
  <span class="st">"classif.rpart.minsplit"</span>,
  <span class="st">"classif.ce"</span>
<span class="op">)</span>, with <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span></code></pre></div>
<pre><code>##     subsample.frac classif.rpart.cp classif.rpart.minsplit classif.ce
##  1:         0.1111          0.02533                      3       0.04
##  2:         0.1111          0.07348                      5       0.02
##  3:         0.1111          0.08490                      3       0.02
##  4:         0.1111          0.05026                      6       0.02
##  5:         0.1111          0.03940                      4       0.02
##  6:         0.1111          0.02540                      7       0.42
##  7:         0.1111          0.01200                      4       0.14
##  8:         0.1111          0.03961                      4       0.02
##  9:         0.1111          0.05762                      6       0.02
## 10:         0.3333          0.07348                      5       0.06
## 11:         0.3333          0.08490                      3       0.04
## 12:         0.3333          0.05026                      6       0.06
## 13:         1.0000          0.08490                      3       0.04
## 14:         0.3333          0.08650                      6       0.02
## 15:         0.3333          0.07491                      9       0.06
## 16:         0.3333          0.06716                      6       0.04
## 17:         0.3333          0.06218                      9       0.08
## 18:         0.3333          0.03785                      4       0.06
## 19:         1.0000          0.08650                      6       0.04
## 20:         1.0000          0.02724                     10       0.04
## 21:         1.0000          0.05689                      3       0.04
## 22:         1.0000          0.09141                      4       0.04
##     subsample.frac classif.rpart.cp classif.rpart.minsplit classif.ce</code></pre>
<p>You can access the best found configuration through the instance object.</p>
<div class="sourceCode" id="cb282"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result</span></code></pre></div>
<pre><code>##    classif.rpart.cp classif.rpart.minsplit subsample.frac learner_param_vals
## 1:          0.07348                      5         0.1111          &lt;list[6]&gt;
##     x_domain classif.ce
## 1: &lt;list[3]&gt;       0.02</code></pre>
<div class="sourceCode" id="cb284"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result_learner_param_vals</span></code></pre></div>
<pre><code>## $subsample.frac
## [1] 0.1111
## 
## $subsample.stratify
## [1] FALSE
## 
## $subsample.replace
## [1] FALSE
## 
## $classif.rpart.xval
## [1] 0
## 
## $classif.rpart.cp
## [1] 0.07348
## 
## $classif.rpart.minsplit
## [1] 5</code></pre>
<div class="sourceCode" id="cb286"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result_y</span></code></pre></div>
<pre><code>## classif.ce 
##       0.02</code></pre>
<p>If you are familiar with the original paper, you may have wondered how we just used Hyperband with a parameter ranging from <code>0.1</code> to <code>1.0</code> <span class="citation">(<a href="references.html#ref-Li2016" role="doc-biblioref">Li et al. 2016</a>)</span>.
The answer is, with the help the internal rescaling of the budget parameter.
<a href="https://mlr3hyperband.mlr-org.com">mlr3hyperband</a> automatically divides the budget parameters boundaries with its lower bound, ending up with a budget range starting again at <code>1</code>, like it is the case originally.
If we want an overview of what bracket layout Hyperband created and how the rescaling in each bracket worked, we can print a compact table to see this information.</p>
<div class="sourceCode" id="cb288"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/duplicated.html">unique</a></span><span class="op">(</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">bracket</span>, <span class="va">bracket_stage</span>, <span class="va">budget_scaled</span>, <span class="va">budget_real</span>, <span class="va">n_configs</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<pre><code>##    bracket bracket_stage budget_scaled budget_real n_configs
## 1:       2             0         1.111      0.1111         9
## 2:       2             1         3.333      0.3333         3
## 3:       2             2        10.000      1.0000         1
## 4:       1             0         3.333      0.3333         5
## 5:       1             1        10.000      1.0000         1
## 6:       0             0        10.000      1.0000         3</code></pre>
<p>In the traditional way, Hyperband uses uniform sampling to receive a configuration sample at the start of each bracket.
But it is also possible to define a custom <a href="https://paradox.mlr-org.com/reference/Sampler.html"><code>Sampler</code></a> for each hyperparameter.</p>
<div class="sourceCode" id="cb290"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">search_space</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>
  nrounds <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_int</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fl">16</span>, tags <span class="op">=</span> <span class="st">"budget"</span><span class="op">)</span>,
  eta <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,
  booster <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_fct</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gbtree"</span>, <span class="st">"gblinear"</span>, <span class="st">"dart"</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span>

<span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html">TuningInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  task <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span>,
  learner <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.xgboost"</span><span class="op">)</span>,
  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,
  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,
  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"none"</span><span class="op">)</span>, <span class="co"># hyperband terminates itself</span>
  search_space <span class="op">=</span> <span class="va">search_space</span>
<span class="op">)</span>

<span class="co"># beta distribution with alpha = 2 and beta = 5</span>
<span class="co"># categorical distribution with custom probabilities</span>
<span class="va">sampler</span> <span class="op">=</span> <span class="va">SamplerJointIndep</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
  <span class="va">Sampler1DRfun</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">search_space</span><span class="op">$</span><span class="va">params</span><span class="op">$</span><span class="va">eta</span>, <span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html">rbeta</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">2</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span>,
  <span class="va">Sampler1DCateg</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">search_space</span><span class="op">$</span><span class="va">params</span><span class="op">$</span><span class="va">booster</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>
<span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Then, the defined sampler has to be given as an argument during instance creation.
Afterwards, the usual tuning can proceed.</p>
<div class="sourceCode" id="cb291"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"hyperband"</span>, eta <span class="op">=</span> <span class="fl">2</span>, sampler <span class="op">=</span> <span class="va">sampler</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></code></pre></div>
<pre><code>## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:55] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:57] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:58] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:58] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:58] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:58] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:59] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:59] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:04:59] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:00] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:01] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:01] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:01] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:01] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:01] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:02] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:02] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:03] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:04] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:04] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:04] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:05] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:05] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:05] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:05] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:05] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:05] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:06] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:06] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:07] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:07] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:07] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:07] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:07] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.</code></pre>
<pre><code>##    nrounds    eta booster learner_param_vals  x_domain classif.ce
## 1:       1 0.2415    dart          &lt;list[5]&gt; &lt;list[3]&gt;       0.04</code></pre>
<div class="sourceCode" id="cb294"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result</span></code></pre></div>
<pre><code>##    nrounds    eta booster learner_param_vals  x_domain classif.ce
## 1:       1 0.2415    dart          &lt;list[5]&gt; &lt;list[3]&gt;       0.04</code></pre>
<p>Furthermore, we extended the original algorithm, to make it also possible to use <a href="https://mlr3hyperband.mlr-org.com">mlr3hyperband</a> for multi-objective optimization.
To do this, simply specify more measures in the <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html"><code>TuningInstanceMultiCrit</code></a> and run the rest as usual.</p>
<div class="sourceCode" id="cb296"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html">TuningInstanceMultiCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  task <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"pima"</span><span class="op">)</span>,
  learner <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.xgboost"</span><span class="op">)</span>,
  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,
  measures <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msrs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.tpr"</span>, <span class="st">"classif.fpr"</span><span class="op">)</span><span class="op">)</span>,
  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"none"</span><span class="op">)</span>, <span class="co"># hyperband terminates itself</span>
  search_space <span class="op">=</span> <span class="va">search_space</span>
<span class="op">)</span>

<span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"hyperband"</span>, eta <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>
<span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></code></pre></div>
<pre><code>## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
## [12:05:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.</code></pre>
<pre><code>##     nrounds     eta  booster learner_param_vals  x_domain classif.tpr
##  1:       1 0.34093 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  2:       1 0.50700 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  3:       1 0.77369 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  4:       1 0.55967 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  5:       1 0.46843 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  6:       1 0.27922 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  7:       1 0.79274 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  8:       1 0.58557 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  9:       1 0.39476 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
## 10:       1 0.58145 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
## 11:      16 0.01919   gbtree          &lt;list[5]&gt; &lt;list[3]&gt;      0.6374
## 12:       4 0.55531     dart          &lt;list[5]&gt; &lt;list[3]&gt;      0.5824
## 13:       4 0.65920   gbtree          &lt;list[5]&gt; &lt;list[3]&gt;      0.6044
## 14:      16 0.13587     dart          &lt;list[5]&gt; &lt;list[3]&gt;      0.6264
##     classif.fpr
##  1:      0.0000
##  2:      0.0000
##  3:      0.0000
##  4:      0.0000
##  5:      0.0000
##  6:      0.0000
##  7:      0.0000
##  8:      0.0000
##  9:      0.0000
## 10:      0.0000
## 11:      0.2364
## 12:      0.1576
## 13:      0.1758
## 14:      0.1818</code></pre>
<p>Now the result is not a single best configuration but an estimated Pareto front.
All red points are not dominated by another parameter configuration regarding their <em>fpr</em> and <em>tpr</em> performance measures.</p>
<div class="sourceCode" id="cb299"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result</span></code></pre></div>
<pre><code>##     nrounds     eta  booster learner_param_vals  x_domain classif.tpr
##  1:       1 0.34093 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  2:       1 0.50700 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  3:       1 0.77369 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  4:       1 0.55967 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  5:       1 0.46843 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  6:       1 0.27922 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  7:       1 0.79274 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  8:       1 0.58557 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
##  9:       1 0.39476 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
## 10:       1 0.58145 gblinear          &lt;list[5]&gt; &lt;list[3]&gt;      0.0000
## 11:      16 0.01919   gbtree          &lt;list[5]&gt; &lt;list[3]&gt;      0.6374
## 12:       4 0.55531     dart          &lt;list[5]&gt; &lt;list[3]&gt;      0.5824
## 13:       4 0.65920   gbtree          &lt;list[5]&gt; &lt;list[3]&gt;      0.6044
## 14:      16 0.13587     dart          &lt;list[5]&gt; &lt;list[3]&gt;      0.6264
##     classif.fpr
##  1:      0.0000
##  2:      0.0000
##  3:      0.0000
##  4:      0.0000
##  5:      0.0000
##  6:      0.0000
##  7:      0.0000
##  8:      0.0000
##  9:      0.0000
## 10:      0.0000
## 11:      0.2364
## 12:      0.1576
## 13:      0.1758
## 14:      0.1818</code></pre>
<div class="sourceCode" id="cb301"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">classif.tpr</span><span class="op">~</span><span class="va">classif.fpr</span>, <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">classif.tpr</span><span class="op">~</span><span class="va">classif.fpr</span>, <span class="va">instance</span><span class="op">$</span><span class="va">result</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="03-optimization-hyperband_files/figure-html/03-optimization-hyperband-013-1.svg" width="672" style="display: block; margin: auto;"></div>

</div>
<div id="fs" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> Feature Selection / Filtering<a class="anchor" aria-label="anchor" href="#fs"><i class="fas fa-link"></i></a>
</h2>
<p>Often, data sets include a large number of features.
The technique of extracting a subset of relevant features is called “feature selection”.</p>
<p>The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.
Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance.
Different approaches exist to identify the relevant features.
Two different approaches are emphasized in the literature:
one is called <a href="optimization.html#fs-filter">Filtering</a> and the other approach is often referred to as feature subset selection or <a href="optimization.html#fs-wrapper">wrapper methods</a>.</p>
<p>What are the differences <span class="citation">(<a href="references.html#ref-guyon2003" role="doc-biblioref">Guyon and Elisseeff 2003</a>; <a href="references.html#ref-chandrashekar2014" role="doc-biblioref">Chandrashekar and Sahin 2014</a>)</span>?</p>
<ul>
<li><p><strong>Filtering</strong>:
An external algorithm computes a rank of the features (e.g. based on the correlation to the response).
Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables.
The selected features will then be used to fit a model (with optional hyperparameters selected by tuning).
This calculation is usually cheaper than “feature subset selection” in terms of computation time.
All filters are connected via package <a href="https://mlr3filters.mlr-org.com">mlr3filters</a>.</p></li>
<li><p><strong>Wrapper Methods</strong>:
Here, no ranking of features is done.
Instead, an optimization algorithm selects a subset of the features, evaluates the set by calculating the resampled predictive performance, and then
proposes a new set of features (or terminates).
A simple example is the sequential forward selection.
This method is usually computationally very intensive as a lot of models are fitted.
Also, strictly speaking, all these models would need to be tuned before the performance is estimated.
This would require an additional nested level in a CV setting.
After undertaken all of these steps, the final set of selected features is again fitted (with optional hyperparameters selected by tuning).
Wrapper methods are implemented in the <a href="https://mlr3fselect.mlr-org.com">mlr3fselect</a> package.</p></li>
<li>
<p><strong>Embedded Methods</strong>:
Many learners internally select a subset of the features which they find helpful for prediction.
These subsets can usually be queried, as the following example demonstrates:</p>
<div class="sourceCode" id="cb302"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://mlr3verse.mlr-org.com">"mlr3verse"</a></span><span class="op">)</span>

<span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span>
<span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span>

<span class="co"># ensure that the learner selects features</span>
<span class="fu"><a href="https://rdrr.io/r/base/stopifnot.html">stopifnot</a></span><span class="op">(</span><span class="st">"selected_features"</span> <span class="op">%in%</span> <span class="va">learner</span><span class="op">$</span><span class="va">properties</span><span class="op">)</span>

<span class="co"># fit a simple classification tree</span>
<span class="va">learner</span> <span class="op">=</span> <span class="va">learner</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span><span class="op">)</span>

<span class="co"># extract all features used in the classification tree:</span>
<span class="va">learner</span><span class="op">$</span><span class="fu">selected_features</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "Petal.Length" "Petal.Width"</code></pre>
</li>
</ul>
<p>There are also ensemble filters built upon the idea of stacking single filter methods. These are not yet implemented.</p>
<div id="fs-filter" class="section level3" number="3.5.1">
<h3>
<span class="header-section-number">3.5.1</span> Filters<a class="anchor" aria-label="anchor" href="#fs-filter"><i class="fas fa-link"></i></a>
</h3>
<p>Filter methods assign an importance value to each feature.
Based on these values the features can be ranked.
Thereafter, we are able to select a feature subset.
There is a list of all implemented filter methods in the <a href="appendix.html#list-filters">Appendix</a>.</p>
</div>
<div id="fs-calc" class="section level3" number="3.5.2">
<h3>
<span class="header-section-number">3.5.2</span> Calculating filter values<a class="anchor" aria-label="anchor" href="#fs-calc"><i class="fas fa-link"></i></a>
</h3>
<p>Currently, only classification and regression tasks are supported.</p>
<p>The first step it to create a new R object using the class of the desired filter method.
Similar to other instances in <code>mlr3</code>, these are registered in a dictionary (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters.html"><code>mlr_filters</code></a>) with an associated shortcut function <a href="https://mlr3filters.mlr-org.com/reference/flt.html"><code>flt()</code></a>.
Each object of class <code>Filter</code> has a <code>.$calculate()</code> method which computes the filter values and ranks them in a descending order.</p>
<div class="sourceCode" id="cb304"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">filter</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"jmim"</span><span class="op">)</span>

<span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span>
<span class="va">filter</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">task</span><span class="op">)</span>

<span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">filter</span><span class="op">)</span></code></pre></div>
<pre><code>##         feature  score
## 1:  Petal.Width 1.0000
## 2: Sepal.Length 0.6667
## 3: Petal.Length 0.3333
## 4:  Sepal.Width 0.0000</code></pre>
<p>Some filters support changing specific hyperparameters.
This is similar to setting hyperparameters of a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> using <code>.$param_set$values</code>:</p>
<div class="sourceCode" id="cb306"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">filter_cor</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"correlation"</span><span class="op">)</span>
<span class="va">filter_cor</span><span class="op">$</span><span class="va">param_set</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##        id    class lower upper nlevels    default value
## 1:    use ParamFct    NA    NA       5 everything      
## 2: method ParamFct    NA    NA       3    pearson</code></pre>
<div class="sourceCode" id="cb308"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># change parameter 'method'</span>
<span class="va">filter_cor</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"spearman"</span><span class="op">)</span>
<span class="va">filter_cor</span><span class="op">$</span><span class="va">param_set</span></code></pre></div>
<pre><code>## &lt;ParamSet&gt;
##        id    class lower upper nlevels    default    value
## 1:    use ParamFct    NA    NA       5 everything         
## 2: method ParamFct    NA    NA       3    pearson spearman</code></pre>
</div>
<div id="fs-var-imp-filters" class="section level3" number="3.5.3">
<h3>
<span class="header-section-number">3.5.3</span> Variable Importance Filters<a class="anchor" aria-label="anchor" href="#fs-var-imp-filters"><i class="fas fa-link"></i></a>
</h3>
<p>All <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> with the property “importance” come with integrated feature selection methods.</p>
<p>You can find a list of all learners with this property in the <a href="appendix.html#fs-filter-embedded-list">Appendix</a>.</p>
<p>For some learners the desired filter method needs to be set during learner creation.
For example, learner <a href="https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html"><code>classif.ranger</code></a> comes with multiple integrated methods, c.f. the help page of <a href="https://www.rdocumentation.org/packages/ranger/topics/ranger"><code>ranger::ranger()</code></a>.
To use method “impurity”, you need to set the filter method during construction.</p>
<div class="sourceCode" id="cb310"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lrn</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.ranger"</span>, importance <span class="op">=</span> <span class="st">"impurity"</span><span class="op">)</span></code></pre></div>
<p>Now you can use the <a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_importance.html"><code>FilterImportance</code></a> filter class for algorithm-embedded methods:</p>
<div class="sourceCode" id="cb311"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"iris"</span><span class="op">)</span>
<span class="va">filter</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"importance"</span>, learner <span class="op">=</span> <span class="va">lrn</span><span class="op">)</span>
<span class="va">filter</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">task</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">filter</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span></code></pre></div>
<pre><code>##         feature  score
## 1: Petal.Length 45.661
## 2:  Petal.Width 42.053
## 3: Sepal.Length  9.322</code></pre>
</div>
<div id="fs-wrapper" class="section level3" number="3.5.4">
<h3>
<span class="header-section-number">3.5.4</span> Wrapper Methods<a class="anchor" aria-label="anchor" href="#fs-wrapper"><i class="fas fa-link"></i></a>
</h3>
<p>Wrapper feature selection is supported via the <a href="https://mlr3fselect.mlr-org.com">mlr3fselect</a> extension package.
At the heart of <a href="https://mlr3fselect.mlr-org.com">mlr3fselect</a> are the R6 classes:</p>
<ul>
<li>
<a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html"><code>FSelectInstanceSingleCrit</code></a>, <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceMultiCrit.html"><code>FSelectInstanceMultiCrit</code></a>: These two classes describe the feature selection problem and store the results.</li>
<li>
<a href="https://mlr3fselect.mlr-org.com/reference/FSelector.html"><code>FSelector</code></a>: This class is the base class for implementations of feature selection algorithms.</li>
</ul>
</div>
<div id="fs-wrapper-optimization" class="section level3" number="3.5.5">
<h3>
<span class="header-section-number">3.5.5</span> The <code>FSelectInstance</code> Classes<a class="anchor" aria-label="anchor" href="#fs-wrapper-optimization"><i class="fas fa-link"></i></a>
</h3>
<p>The following sub-section examines the feature selection on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html"><code>Pima</code></a> data set which is used to predict whether or not a patient has diabetes.</p>
<div class="sourceCode" id="cb313"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"pima"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">task</span><span class="op">)</span></code></pre></div>
<pre><code>## &lt;TaskClassif:pima&gt; (768 x 9)
## * Target: diabetes
## * Properties: twoclass
## * Features (8):
##   - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,
##     triceps</code></pre>
<p>We use the classification tree from <a href="https://cran.r-project.org/package=rpart">rpart</a>.</p>
<div class="sourceCode" id="cb315"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></code></pre></div>
<p>Next, we need to specify how to evaluate the performance of the feature subsets.
For this, we need to choose a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>resampling strategy</code></a> and a <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>performance measure</code></a>.</p>
<div class="sourceCode" id="cb316"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">hout</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>
<span class="va">measure</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span></code></pre></div>
<p>Finally, one has to choose the available budget for the feature selection.
This is done by selecting one of the available <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminators</code></a>:</p>
<ul>
<li>Terminate after a given time (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_clock_time.html"><code>TerminatorClockTime</code></a>)</li>
<li>Terminate after a given amount of iterations (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_evals.html"><code>TerminatorEvals</code></a>)</li>
<li>Terminate after a specific performance is reached (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_perf_reached.html"><code>TerminatorPerfReached</code></a>)</li>
<li>Terminate when feature selection does not improve (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_stagnation.html"><code>TerminatorStagnation</code></a>)</li>
<li>A combination of the above in an <em>ALL</em> or <em>ANY</em> fashion (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html"><code>TerminatorCombo</code></a>)</li>
</ul>
<p>For this short introduction, we specify a budget of 20 evaluations and then put everything together into a <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html"><code>FSelectInstanceSingleCrit</code></a>:</p>
<div class="sourceCode" id="cb317"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">evals20</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span>

<span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html">FSelectInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  task <span class="op">=</span> <span class="va">task</span>,
  learner <span class="op">=</span> <span class="va">learner</span>,
  resampling <span class="op">=</span> <span class="va">hout</span>,
  measure <span class="op">=</span> <span class="va">measure</span>,
  terminator <span class="op">=</span> <span class="va">evals20</span>
<span class="op">)</span>
<span class="va">instance</span></code></pre></div>
<pre><code>## &lt;FSelectInstanceSingleCrit&gt;
## * State:  Not optimized
## * Objective: &lt;ObjectiveFSelect:classif.rpart_on_pima&gt;
## * Search Space:
## &lt;ParamSet&gt;
##          id    class lower upper nlevels        default value
## 1:      age ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 2:  glucose ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 3:  insulin ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 4:     mass ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 5: pedigree ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 6: pregnant ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 7: pressure ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## 8:  triceps ParamLgl    NA    NA       2 &lt;NoDefault[3]&gt;      
## * Terminator: &lt;TerminatorEvals&gt;
## * Terminated: FALSE
## * Archive:
## &lt;ArchiveFSelect&gt;
## Null data.table (0 rows and 0 cols)</code></pre>
<p>To start the feature selection, we still need to select an algorithm which are defined via the <a href="https://mlr3fselect.mlr-org.com/reference/FSelector.html"><code>FSelector</code></a> class</p>
</div>
<div id="the-fselector-class" class="section level3" number="3.5.6">
<h3>
<span class="header-section-number">3.5.6</span> The <code>FSelector</code> Class<a class="anchor" aria-label="anchor" href="#the-fselector-class"><i class="fas fa-link"></i></a>
</h3>
<p>The following algorithms are currently implemented in <a href="https://mlr3fselect.mlr-org.com">mlr3fselect</a>:</p>
<ul>
<li>Random Search (<a href="https://mlr3fselect.mlr-org.com/reference/FSelectorRandomSearch.html"><code>FSelectorRandomSearch</code></a>)</li>
<li>Exhaustive Search (<a href="https://mlr3fselect.mlr-org.com/reference/FSelectorExhaustiveSearch.html"><code>FSelectorExhaustiveSearch</code></a>)</li>
<li>Sequential Search (<a href="https://mlr3fselect.mlr-org.com/reference/FSelectorSequential.html"><code>FSelectorSequential</code></a>)</li>
<li>Recursive Feature Elimination (<a href="https://mlr3fselect.mlr-org.com/reference/FSelectorRFE.html"><code>FSelectorRFE</code></a>)</li>
<li>Design Points (<a href="https://mlr3fselect.mlr-org.com/reference/FSelectorDesignPoints.html"><code>FSelectorDesignPoints</code></a>)</li>
</ul>
<p>In this example, we will use a simple random search and retrieve it from the dictionary <a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors.html"><code>mlr_fselectors</code></a> with the <a href="https://mlr3fselect.mlr-org.com/reference/fs.html"><code>fs()</code></a> function:</p>
<div class="sourceCode" id="cb319"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fselector</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></code></pre></div>
</div>
<div id="wrapper-selection-triggering" class="section level3" number="3.5.7">
<h3>
<span class="header-section-number">3.5.7</span> Triggering the Tuning<a class="anchor" aria-label="anchor" href="#wrapper-selection-triggering"><i class="fas fa-link"></i></a>
</h3>
<p>To start the feature selection, we simply pass the <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html"><code>FSelectInstanceSingleCrit</code></a> to the <code>$optimize()</code> method of the initialized <a href="https://mlr3fselect.mlr-org.com/reference/FSelector.html"><code>FSelector</code></a>. The algorithm proceeds as follows</p>
<ol style="list-style-type: decimal">
<li>The <a href="https://mlr3fselect.mlr-org.com/reference/FSelector.html"><code>FSelector</code></a> proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting <code>batch_size</code>).</li>
<li>For each feature subset, the given <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> is fitted on the <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a> using the provided <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a>.
All evaluations are stored in the archive of the <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html"><code>FSelectInstanceSingleCrit</code></a>.</li>
<li>The <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminator</code></a> is queried if the budget is exhausted.
If the budget is not exhausted, restart with 1) until it is.</li>
<li>Determine the feature subset with the best observed performance.</li>
<li>Store the best feature subset as the result in the instance object.
The best feature subset (<code>$result_feature_set</code>) and the corresponding measured performance (<code>$result_y</code>) can be accessed from the instance.</li>
</ol>
<div class="sourceCode" id="cb320"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># reduce logging output</span>
<span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"bbotk"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"warn"</span><span class="op">)</span>

<span class="va">fselector</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></code></pre></div>
<pre><code>##     age glucose insulin mass pedigree pregnant pressure triceps
## 1: TRUE    TRUE   FALSE TRUE     TRUE     TRUE    FALSE   FALSE
##                              features classif.ce
## 1: age,glucose,mass,pedigree,pregnant       0.25</code></pre>
<div class="sourceCode" id="cb322"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result_feature_set</span></code></pre></div>
<pre><code>## [1] "age"      "glucose"  "mass"     "pedigree" "pregnant"</code></pre>
<div class="sourceCode" id="cb324"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">result_y</span></code></pre></div>
<pre><code>## classif.ce 
##       0.25</code></pre>
<p>One can investigate all resamplings which were undertaken, as they are stored in the archive of the <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html"><code>FSelectInstanceSingleCrit</code></a> and can be accessed by using <code><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table()</a></code>:</p>
<div class="sourceCode" id="cb326"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span></code></pre></div>
<pre><code>##       age glucose insulin  mass pedigree pregnant pressure triceps classif.ce
##  1:  TRUE   FALSE    TRUE FALSE     TRUE     TRUE    FALSE   FALSE     0.3203
##  2:  TRUE    TRUE   FALSE FALSE    FALSE    FALSE     TRUE    TRUE     0.2617
##  3: FALSE   FALSE   FALSE FALSE    FALSE    FALSE    FALSE    TRUE     0.4297
##  4:  TRUE    TRUE   FALSE  TRUE     TRUE     TRUE    FALSE   FALSE     0.2500
##  5: FALSE    TRUE   FALSE FALSE    FALSE    FALSE     TRUE   FALSE     0.2695
##  6: FALSE    TRUE    TRUE FALSE    FALSE    FALSE    FALSE    TRUE     0.2812
##  7:  TRUE   FALSE    TRUE FALSE     TRUE     TRUE     TRUE    TRUE     0.3086
##  8: FALSE    TRUE    TRUE  TRUE    FALSE     TRUE     TRUE    TRUE     0.2539
##  9:  TRUE   FALSE   FALSE  TRUE    FALSE    FALSE    FALSE    TRUE     0.2891
## 10:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE     0.2734
## 11:  TRUE   FALSE    TRUE FALSE    FALSE    FALSE    FALSE    TRUE     0.3320
## 12: FALSE   FALSE    TRUE  TRUE    FALSE    FALSE    FALSE   FALSE     0.3359
## 13:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE     0.2734
## 14: FALSE   FALSE   FALSE  TRUE    FALSE    FALSE    FALSE   FALSE     0.3984
## 15:  TRUE    TRUE   FALSE FALSE    FALSE    FALSE    FALSE   FALSE     0.2578
## 16:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE     0.2734
## 17: FALSE    TRUE    TRUE FALSE    FALSE     TRUE     TRUE    TRUE     0.2773
## 18:  TRUE   FALSE    TRUE  TRUE    FALSE    FALSE     TRUE   FALSE     0.2930
## 19: FALSE    TRUE   FALSE  TRUE     TRUE    FALSE     TRUE   FALSE     0.2539
## 20:  TRUE    TRUE    TRUE  TRUE    FALSE     TRUE     TRUE    TRUE     0.2656
##     runtime_learners           timestamp batch_nr      resample_result
##  1:            0.082 2021-10-05 12:05:20        1 &lt;ResampleResult[20]&gt;
##  2:            0.068 2021-10-05 12:05:20        2 &lt;ResampleResult[20]&gt;
##  3:            0.070 2021-10-05 12:05:21        3 &lt;ResampleResult[20]&gt;
##  4:            0.065 2021-10-05 12:05:21        4 &lt;ResampleResult[20]&gt;
##  5:            0.079 2021-10-05 12:05:21        5 &lt;ResampleResult[20]&gt;
##  6:            0.085 2021-10-05 12:05:21        6 &lt;ResampleResult[20]&gt;
##  7:            0.073 2021-10-05 12:05:22        7 &lt;ResampleResult[20]&gt;
##  8:            0.070 2021-10-05 12:05:22        8 &lt;ResampleResult[20]&gt;
##  9:            0.083 2021-10-05 12:05:22        9 &lt;ResampleResult[20]&gt;
## 10:            0.070 2021-10-05 12:05:22       10 &lt;ResampleResult[20]&gt;
## 11:            0.075 2021-10-05 12:05:23       11 &lt;ResampleResult[20]&gt;
## 12:            0.079 2021-10-05 12:05:23       12 &lt;ResampleResult[20]&gt;
## 13:            0.072 2021-10-05 12:05:23       13 &lt;ResampleResult[20]&gt;
## 14:            0.071 2021-10-05 12:05:23       14 &lt;ResampleResult[20]&gt;
## 15:            0.073 2021-10-05 12:05:24       15 &lt;ResampleResult[20]&gt;
## 16:            0.066 2021-10-05 12:05:24       16 &lt;ResampleResult[20]&gt;
## 17:            0.075 2021-10-05 12:05:24       17 &lt;ResampleResult[20]&gt;
## 18:            0.067 2021-10-05 12:05:24       18 &lt;ResampleResult[20]&gt;
## 19:            0.074 2021-10-05 12:05:25       19 &lt;ResampleResult[20]&gt;
## 20:            0.361 2021-10-05 12:05:25       20 &lt;ResampleResult[20]&gt;</code></pre>
<p>The associated resampling iterations can be accessed in the <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a>:</p>
<div class="sourceCode" id="cb328"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">benchmark_result</span><span class="op">$</span><span class="va">data</span></code></pre></div>
<pre><code>## Warning: '.__BenchmarkResult__data' is deprecated.
## Use 'as.data.table(benchmark_result)' instead.
## See help("Deprecated")</code></pre>
<pre><code>## &lt;ResultData&gt;
##   Public:
##     as_data_table: function (view = NULL, reassemble_learners = TRUE, convert_predictions = TRUE, 
##     clone: function (deep = FALSE) 
##     combine: function (rdata) 
##     data: list
##     initialize: function (data = NULL, store_backends = TRUE) 
##     iterations: function (view = NULL) 
##     learners: function (view = NULL, states = TRUE, reassemble = TRUE) 
##     logs: function (view = NULL, condition) 
##     prediction: function (view = NULL, predict_sets = "test") 
##     predictions: function (view = NULL, predict_sets = "test") 
##     resamplings: function (view = NULL) 
##     sweep: function () 
##     task_type: active binding
##     tasks: function (view = NULL) 
##     uhashes: function (view = NULL) 
##   Private:
##     deep_clone: function (name, value) 
##     get_view_index: function (view)</code></pre>
<p>The <code>uhash</code> column links the resampling iterations to the evaluated feature subsets stored in <code>instance$archive$data()</code>. This allows e.g. to score the included <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a>s on a different measure.</p>
<p>Now the optimized feature subset can be used to subset the task and fit the model on all observations.</p>
<div class="sourceCode" id="cb331"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">task</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">result_feature_set</span><span class="op">)</span>
<span class="va">learner</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></code></pre></div>
<p>The trained model can now be used to make a prediction on external data.
Note that predicting on observations present in the <code>task</code>, should be avoided.
The model has seen these observations already during feature selection and therefore results would be statistically biased.
Hence, the resulting performance measure would be over-optimistic.
Instead, to get statistically unbiased performance estimates for the current task, <a href="optimization.html#nested-resampling">nested resampling</a> is required.</p>
</div>
<div id="autofselect" class="section level3" number="3.5.8">
<h3>
<span class="header-section-number">3.5.8</span> Automating the Feature Selection<a class="anchor" aria-label="anchor" href="#autofselect"><i class="fas fa-link"></i></a>
</h3>
<p>The <a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a> wraps a learner and augments it with an automatic feature selection for a given task.
Because the <a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a> itself inherits from the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> base class, it can be used like any other learner.
Analogously to the previous subsection, a new classification tree learner is created.
This classification tree learner automatically starts a feature selection on the given task using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and uses a simple random search as feature selection algorithm:</p>
<div class="sourceCode" id="cb332"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">learner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span>
<span class="va">terminator</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="va">fselector</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span>

<span class="va">at</span> <span class="op">=</span> <span class="va"><a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html">AutoFSelector</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  learner <span class="op">=</span> <span class="va">learner</span>,
  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,
  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,
  terminator <span class="op">=</span> <span class="va">terminator</span>,
  fselector <span class="op">=</span> <span class="va">fselector</span>
<span class="op">)</span>
<span class="va">at</span></code></pre></div>
<pre><code>## &lt;AutoFSelector:classif.rpart.fselector&gt;
## * Model: -
## * Parameters: xval=0
## * Packages: rpart
## * Predict Type: response
## * Feature types: logical, integer, numeric, factor, ordered
## * Properties: importance, missings, multiclass, selected_features,
##   twoclass, weights</code></pre>
<p>We can now use the learner like any other learner, calling the <code>$train()</code> and <code>$predict()</code> method.
This time however, we pass it to <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> to compare the optimized feature subset to the complete feature set.
This way, the <a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a> will do its resampling for feature selection on the training set of the respective split of the outer resampling.
The learner then undertakes predictions using the test set of the outer resampling.
This yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.
This is called <a href="optimization.html#nested-resampling">nested resampling</a>.</p>
<p>To compare the optimized feature subset with the complete feature set, we can use <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a>:</p>
<div class="sourceCode" id="cb334"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">grid</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html">benchmark_grid</a></span><span class="op">(</span>
  task <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"pima"</span><span class="op">)</span>,
  learner <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">at</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span><span class="op">)</span>,
  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
<span class="op">)</span>

<span class="va">bmr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark</a></span><span class="op">(</span><span class="va">grid</span>, store_models <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msrs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.ce"</span>, <span class="st">"time_train"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    nr      resample_result task_id              learner_id resampling_id iters
## 1:  1 &lt;ResampleResult[20]&gt;    pima classif.rpart.fselector            cv     3
## 2:  2 &lt;ResampleResult[20]&gt;    pima           classif.rpart            cv     3
##    classif.ce time_train
## 1:     0.2422          0
## 2:     0.2448          0</code></pre>
<p>Note that we do not expect any significant differences since we only evaluated a small fraction of the possible feature subsets.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="basics.html"><span class="header-section-number">2</span> Basics</a></div>
<div class="next"><a href="pipelines.html"><span class="header-section-number">4</span> Pipelines</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#optimization"><span class="header-section-number">3</span> Model Optimization</a></li>
<li>
<a class="nav-link" href="#tuning"><span class="header-section-number">3.1</span> Hyperparameter Tuning</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#tuning-optimization"><span class="header-section-number">3.1.1</span> The TuningInstance* Classes</a></li>
<li><a class="nav-link" href="#tuning-algorithms"><span class="header-section-number">3.1.2</span> The Tuner Class</a></li>
<li><a class="nav-link" href="#tuning-triggering"><span class="header-section-number">3.1.3</span> Triggering the Tuning</a></li>
<li><a class="nav-link" href="#autotuner"><span class="header-section-number">3.1.4</span> Automating the Tuning</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#searchspace"><span class="header-section-number">3.2</span> Tuning Search Spaces</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#creating-paramsets"><span class="header-section-number">3.2.1</span> Creating ParamSets</a></li>
<li><a class="nav-link" href="#searchspace-trafo"><span class="header-section-number">3.2.2</span> Transformations (trafo)</a></li>
<li><a class="nav-link" href="#autolevel"><span class="header-section-number">3.2.3</span> Automatic Factor Level Transformation</a></li>
<li><a class="nav-link" href="#searchspace-depends"><span class="header-section-number">3.2.4</span> Parameter Dependencies (depends)</a></li>
<li><a class="nav-link" href="#creating-tuning-paramsets-from-other-paramsets"><span class="header-section-number">3.2.5</span> Creating Tuning ParamSets from other ParamSets</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#nested-resampling"><span class="header-section-number">3.3</span> Nested Resampling</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#nested-resamp-exec"><span class="header-section-number">3.3.1</span> Execution</a></li>
<li><a class="nav-link" href="#nested-resamp-eval"><span class="header-section-number">3.3.2</span> Evaluation</a></li>
<li><a class="nav-link" href="#nested-final-model"><span class="header-section-number">3.3.3</span> Final Model</a></li>
</ul>
</li>
<li><a class="nav-link" href="#hyperband"><span class="header-section-number">3.4</span> Tuning with Hyperband</a></li>
<li>
<a class="nav-link" href="#fs"><span class="header-section-number">3.5</span> Feature Selection / Filtering</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#fs-filter"><span class="header-section-number">3.5.1</span> Filters</a></li>
<li><a class="nav-link" href="#fs-calc"><span class="header-section-number">3.5.2</span> Calculating filter values</a></li>
<li><a class="nav-link" href="#fs-var-imp-filters"><span class="header-section-number">3.5.3</span> Variable Importance Filters</a></li>
<li><a class="nav-link" href="#fs-wrapper"><span class="header-section-number">3.5.4</span> Wrapper Methods</a></li>
<li><a class="nav-link" href="#fs-wrapper-optimization"><span class="header-section-number">3.5.5</span> The FSelectInstance Classes</a></li>
<li><a class="nav-link" href="#the-fselector-class"><span class="header-section-number">3.5.6</span> The FSelector Class</a></li>
<li><a class="nav-link" href="#wrapper-selection-triggering"><span class="header-section-number">3.5.7</span> Triggering the Tuning</a></li>
<li><a class="nav-link" href="#autofselect"><span class="header-section-number">3.5.8</span> Automating the Feature Selection</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/mlr-org/mlr3book/blob/main/bookdown/03-optimization.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/mlr-org/mlr3book/edit/main/bookdown/03-optimization.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>mlr3 book</strong>" was written by Marc Becker, Martin Binder, Bernd Bischl, Michel Lang, Florian Pfisterer, Nicholas G. Reich, Jakob Richter, Patrick Schratz, Raphael Sonabend, Damir Pulatov. It was last built on 2021-10-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
